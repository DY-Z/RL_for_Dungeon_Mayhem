{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6JXNgoNjsf"
      },
      "source": [
        "# environments/dungeonmayhem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmDG5l7INqSC"
      },
      "source": [
        "## card.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82PlzRx_NRV1"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Callable, Optional, Tuple\n",
        "from itertools import count\n",
        "\n",
        "@dataclass\n",
        "class DungeonMayhem_Card:\n",
        "    attack: int = 0\n",
        "    defend: int = 0\n",
        "    draw: int = 0\n",
        "    heal: int = 0\n",
        "    play: int = 0\n",
        "    power: Optional[Tuple[int, Callable[[Any, Any, Any], None]]] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.player = None\n",
        "        self.card_id = -1\n",
        "        self.action_id = -1\n",
        "        self.action_encoding = None\n",
        "\n",
        "    def discard(self):\n",
        "        if self.player is not None and self.player.health > 0:\n",
        "            self.player.discardpile.append(self)\n",
        "\n",
        "def DestroyDefense_Power(game: Any, player: Any, target: Any):\n",
        "    if len(target.defenses) == 0:\n",
        "        return\n",
        "    target.destroy_defense()\n",
        "\n",
        "def BarbarianDiscardHand_Power(game: Any, player: Any, target: Any):\n",
        "    for p in game.active_players:\n",
        "        p.discard_hand()\n",
        "        p.draw(3)\n",
        "\n",
        "def BarbarianHeal_Power(game: Any, player: Any, target: Any):\n",
        "    for p in game.active_players:\n",
        "        if p != player:\n",
        "            player.heal(1)\n",
        "            p.take_damage(1)\n",
        "\n",
        "def PaladinDestroyAllDefenses_Power(game: Any, player: Any, target: Any):\n",
        "    for p in game.active_players:\n",
        "        while len(p.defenses) > 0:\n",
        "            p.destroy_defense()\n",
        "\n",
        "def PaladinDrawDiscard_Power(game: Any, player: Any, target: Any):\n",
        "    if len(player.discardpile) == 0:\n",
        "        return\n",
        "    pick = 0\n",
        "    card = player.discardpile.pop(pick)\n",
        "    player.hand.append(card)\n",
        "\n",
        "def RogueGainImmunity_Power(game: Any, player: Any, target: Any):\n",
        "    game.active_players.remove(player)\n",
        "    game.immune_players.append(player)\n",
        "    \n",
        "def RogueStealDiscard_Power(game: Any, player: Any, target: Any):\n",
        "    if len(target.discardpile) == 0:\n",
        "        return\n",
        "    card = target.discardpile.pop()\n",
        "    player.hand.append(card)\n",
        "    game.play_card(player, card)\n",
        "\n",
        "def WizardFireball_Power(game: Any, player: Any, target: Any):\n",
        "    for p in game.active_players:\n",
        "        p.take_damage(3)\n",
        "\n",
        "def WizardStealDefense_Power(game: Any, player: Any, target: Any):\n",
        "    if len(target.defenses) == 0:\n",
        "        return\n",
        "    defense = target.defenses.pop()\n",
        "    player.defenses.append(defense)\n",
        "\n",
        "def WizardSwapHealth_Power(game: Any, player: Any, target: Any):\n",
        "    if target.health > 0:\n",
        "        player.health, target.health = target.health, player.health\n",
        "\n",
        "power_counter = count(1)\n",
        "DestroyDefense = (next(power_counter), DestroyDefense_Power)\n",
        "BarbarianDiscardHand = (next(power_counter), BarbarianDiscardHand_Power)\n",
        "BarbarianHeal = (next(power_counter), BarbarianHeal_Power)\n",
        "PaladinDestroyAllDefenses = (next(power_counter), PaladinDestroyAllDefenses_Power)\n",
        "PaladinDrawDiscard = (next(power_counter), PaladinDrawDiscard_Power)\n",
        "RogueGainImmunity = (next(power_counter), RogueGainImmunity_Power)\n",
        "RogueStealDiscard = (next(power_counter), RogueStealDiscard_Power)\n",
        "WizardFireball = (next(power_counter), WizardFireball_Power)\n",
        "WizardStealDefense = (next(power_counter), WizardStealDefense_Power)\n",
        "WizardSwapHealth = (next(power_counter), WizardSwapHealth_Power)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxywgcx7h7gU"
      },
      "source": [
        "## character.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "707u4nLDgi_2"
      },
      "outputs": [],
      "source": [
        "from itertools import count\n",
        "\n",
        "# from environments.dungeonmayhem.card import *\n",
        "\n",
        "class DungeonMayhem_Character:\n",
        "    deck = []\n",
        "    @classmethod\n",
        "    def get_deck(cls):\n",
        "        return list(cls.deck)\n",
        "\n",
        "character_counter = count()\n",
        "\n",
        "class DungeonMayhem_Barbarian(DungeonMayhem_Character):\n",
        "    character_id = next(character_counter)\n",
        "    deck = [\n",
        "        DungeonMayhem_Card(attack = 2), # BrutalPunch(),\n",
        "        DungeonMayhem_Card(attack = 2), # BrutalPunch(),\n",
        "        DungeonMayhem_Card(play = 2), # TwoAxes(),\n",
        "        DungeonMayhem_Card(play = 2), # TwoAxes(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # HeadButt(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # HeadButt(),\n",
        "        DungeonMayhem_Card(attack = 3), # BigAxe(),\n",
        "        DungeonMayhem_Card(attack = 3), # BigAxe(),\n",
        "        DungeonMayhem_Card(attack = 3), # BigAxe(),\n",
        "        DungeonMayhem_Card(attack = 3), # BigAxe(),\n",
        "        DungeonMayhem_Card(attack = 3), # BigAxe(),\n",
        "        DungeonMayhem_Card(attack = 4), # Rage(),\n",
        "        DungeonMayhem_Card(attack = 4), # Rage(),\n",
        "        DungeonMayhem_Card(defend = 3), # Riff(),\n",
        "        DungeonMayhem_Card(defend = 3), # Raff(),\n",
        "        DungeonMayhem_Card(defend = 2), # SpikedShield(),\n",
        "        DungeonMayhem_Card(defend = 1, draw = 1), # BagOfRats(),\n",
        "        DungeonMayhem_Card(draw = 2, heal = 1), # SnackTime(),\n",
        "        DungeonMayhem_Card(draw = 2), # OpenTheArmory(),\n",
        "        DungeonMayhem_Card(draw = 2), # OpenTheArmory(),\n",
        "        DungeonMayhem_Card(draw = 1, heal = 1), # Flex(),\n",
        "        DungeonMayhem_Card(draw = 1, heal = 1), # Flex(),\n",
        "        DungeonMayhem_Card(draw = 1, power = DestroyDefense), # MightyToss(),\n",
        "        DungeonMayhem_Card(draw = 1, power = DestroyDefense), # MightyToss(),\n",
        "        DungeonMayhem_Card(play = 1, power = BarbarianDiscardHand), # BattleRoar(),\n",
        "        DungeonMayhem_Card(play = 1, power = BarbarianDiscardHand), # BattleRoar(),\n",
        "        DungeonMayhem_Card(power = BarbarianHeal), # WhirlingAxes(),\n",
        "        DungeonMayhem_Card(power = BarbarianHeal), # WhirlingAxes(),\n",
        "    ]\n",
        "\n",
        "class DungeonMayhem_Paladin(DungeonMayhem_Character):\n",
        "    character_id = next(character_counter)\n",
        "    deck = [\n",
        "        DungeonMayhem_Card(attack = 3), # ForTheMostJustice(),\n",
        "        DungeonMayhem_Card(attack = 3), # ForTheMostJustice(),\n",
        "        DungeonMayhem_Card(attack = 2), # ForEvenMoreJustice(),\n",
        "        DungeonMayhem_Card(attack = 2), # ForEvenMoreJustice(),\n",
        "        DungeonMayhem_Card(attack = 2), # ForEvenMoreJustice(),\n",
        "        DungeonMayhem_Card(attack = 2), # ForEvenMoreJustice(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # ForJustice(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # ForJustice(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # ForJustice(),\n",
        "        DungeonMayhem_Card(play = 2), # FingerWagOfJudgment(),\n",
        "        DungeonMayhem_Card(play = 2), # FingerWagOfJudgment(),\n",
        "        DungeonMayhem_Card(attack = 3, heal = 1), # DivineSmite(),\n",
        "        DungeonMayhem_Card(attack = 3, heal = 1), # DivineSmite(),\n",
        "        DungeonMayhem_Card(attack = 3, heal = 1), # DivineSmite(),\n",
        "        DungeonMayhem_Card(attack = 2, heal = 1), # FightingWords(),\n",
        "        DungeonMayhem_Card(attack = 2, heal = 1), # FightingWords(),\n",
        "        DungeonMayhem_Card(attack = 2, heal = 1), # FightingWords(),\n",
        "        DungeonMayhem_Card(play = 2), # HighCharisma(),\n",
        "        DungeonMayhem_Card(play = 2), # HighCharisma(),\n",
        "        DungeonMayhem_Card(draw = 2, heal = 1), # CureWounds(),\n",
        "        DungeonMayhem_Card(defend = 1, draw = 1), # SpinningParry(),\n",
        "        DungeonMayhem_Card(defend = 1, draw = 1), # SpinningParry(),\n",
        "        DungeonMayhem_Card(defend = 3), # DivineShield(),\n",
        "        DungeonMayhem_Card(defend = 3), # DivineShield(),\n",
        "        DungeonMayhem_Card(defend = 2), # Fluffly(),\n",
        "        DungeonMayhem_Card(play = 1, power = PaladinDestroyAllDefenses), # BanishingSmite(),\n",
        "        DungeonMayhem_Card(heal = 2, power = PaladinDrawDiscard), # DivineInspiration(),\n",
        "        DungeonMayhem_Card(heal = 2, power = PaladinDrawDiscard), # DivineInspiration(),\n",
        "    ]\n",
        "\n",
        "class DungeonMayhem_Rogue(DungeonMayhem_Character):\n",
        "    character_id = next(character_counter)\n",
        "    deck = [\n",
        "        DungeonMayhem_Card(attack = 3), # AllTheThrownDaggers(),\n",
        "        DungeonMayhem_Card(attack = 3), # AllTheThrownDaggers(),\n",
        "        DungeonMayhem_Card(attack = 3), # AllTheThrownDaggers(),\n",
        "        DungeonMayhem_Card(attack = 2), # TwoThrownDaggers(),\n",
        "        DungeonMayhem_Card(attack = 2), # TwoThrownDaggers(),\n",
        "        DungeonMayhem_Card(attack = 2), # TwoThrownDaggers(),\n",
        "        DungeonMayhem_Card(attack = 2), # TwoThrownDaggers(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # OneThrownDagger(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # OneThrownDagger(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # OneThrownDagger(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # OneThrownDagger(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # OneThrownDagger(),\n",
        "        DungeonMayhem_Card(play = 2), # CunningAction(),\n",
        "        DungeonMayhem_Card(play = 2), # CunningAction(),\n",
        "        DungeonMayhem_Card(play = 1, heal = 1), # StolenPotion(),\n",
        "        DungeonMayhem_Card(play = 1, heal = 1), # StolenPotion(),\n",
        "        DungeonMayhem_Card(draw = 2, heal = 1), # EvenMoreDaggers(),\n",
        "        DungeonMayhem_Card(draw = 1, defend = 1), # WingedSerpent(),\n",
        "        DungeonMayhem_Card(draw = 1, defend = 1), # WingedSerpent(),\n",
        "        DungeonMayhem_Card(defend = 2), # TheGoonSquad(),\n",
        "        DungeonMayhem_Card(defend = 2), # TheGoonSquad(),\n",
        "        DungeonMayhem_Card(defend = 3), # MyLittleFriend(),\n",
        "        DungeonMayhem_Card(power = RogueGainImmunity), # CleverDisguise(),\n",
        "        DungeonMayhem_Card(power = RogueGainImmunity), # CleverDisguise(),\n",
        "        DungeonMayhem_Card(play = 1, power = DestroyDefense), # SneakAttack(),\n",
        "        DungeonMayhem_Card(play = 1, power = DestroyDefense), # SneakAttack(),\n",
        "        DungeonMayhem_Card(play = 1, power = RogueStealDiscard), # PickPocket(),\n",
        "        DungeonMayhem_Card(play = 1, power = RogueStealDiscard), # PickPocket(),\n",
        "    ]\n",
        "\n",
        "class DungeonMayhem_Wizard(DungeonMayhem_Character):\n",
        "    character_id = next(character_counter)\n",
        "    deck = [\n",
        "        DungeonMayhem_Card(attack = 3), # LightningBolt(),\n",
        "        DungeonMayhem_Card(attack = 3), # LightningBolt(),\n",
        "        DungeonMayhem_Card(attack = 3), # LightningBolt(),\n",
        "        DungeonMayhem_Card(attack = 3), # LightningBolt(),\n",
        "        DungeonMayhem_Card(attack = 2), # BurningHands(),\n",
        "        DungeonMayhem_Card(attack = 2), # BurningHands(),\n",
        "        DungeonMayhem_Card(attack = 2), # BurningHands(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # MagicMissile(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # MagicMissile(),\n",
        "        DungeonMayhem_Card(attack = 1, play = 1), # MagicMissile(),\n",
        "        DungeonMayhem_Card(play = 2), # SpeedOfThought(),\n",
        "        DungeonMayhem_Card(play = 2), # SpeedOfThought(),\n",
        "        DungeonMayhem_Card(play = 2), # SpeedOfThought(),\n",
        "        DungeonMayhem_Card(heal = 1, play = 1), # EvilSneer(),\n",
        "        DungeonMayhem_Card(heal = 1, play = 1), # EvilSneer(),\n",
        "        DungeonMayhem_Card(draw = 3), # KnowledgeIsPower(),\n",
        "        DungeonMayhem_Card(draw = 3), # KnowledgeIsPower(),\n",
        "        DungeonMayhem_Card(draw = 3), # KnowledgeIsPower(),\n",
        "        DungeonMayhem_Card(defend = 1, draw = 1), # Shield(),\n",
        "        DungeonMayhem_Card(defend = 1, draw = 1), # Shield(),\n",
        "        DungeonMayhem_Card(defend = 2), # Stoneskin(),\n",
        "        DungeonMayhem_Card(defend = 3), # MirrorImage(),\n",
        "        DungeonMayhem_Card(power = WizardFireball), # Fireball(),\n",
        "        DungeonMayhem_Card(power = WizardFireball), # Fireball(),\n",
        "        DungeonMayhem_Card(power = WizardStealDefense), # Charm(),\n",
        "        DungeonMayhem_Card(power = WizardStealDefense), # Charm(),\n",
        "        DungeonMayhem_Card(power = WizardSwapHealth), # VampiricTouch(),\n",
        "        DungeonMayhem_Card(power = WizardSwapHealth), # VampiricTouch(),        \n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLStzHP5kWSh"
      },
      "source": [
        "## player.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq45X7gm-RAO"
      },
      "outputs": [],
      "source": [
        "class DungeonMayhem_Player:\n",
        "\n",
        "    def __init__(self, player_id, np_rng):\n",
        "        self.player_id = player_id\n",
        "        self.np_rng = np_rng\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.character = None\n",
        "        self.deck = []\n",
        "        self.hand = []\n",
        "        self.discardpile = []\n",
        "        self.defenses = []\n",
        "        self.health = 10\n",
        "        self.plays = 1\n",
        "\n",
        "    def get_character_deck(self, character):\n",
        "        self.character = character\n",
        "        self.deck = character.get_deck()\n",
        "        self.np_rng.shuffle(self.deck)\n",
        "        for card in self.deck:\n",
        "            card.player = self\n",
        "\n",
        "    def draw(self, n):\n",
        "        for _ in range(n):\n",
        "            if len(self.deck) == 0:\n",
        "                self.deck = self.discardpile\n",
        "                self.discardpile = []\n",
        "                self.np_rng.shuffle(self.deck)\n",
        "            if len(self.deck) == 0:\n",
        "                return\n",
        "            card = self.deck.pop()\n",
        "            self.hand.append(card)\n",
        "\n",
        "    def discard_hand(self):\n",
        "        self.discardpile.extend(self.hand)\n",
        "        self.hand = []\n",
        "\n",
        "    def total_health(self):\n",
        "        return self.health + self.total_defenses()\n",
        "\n",
        "    def total_defenses(self):\n",
        "        return sum(defense[0] for defense in self.defenses)\n",
        "\n",
        "    def start_turn(self):\n",
        "        self.plays = 1\n",
        "        self.draw(1)\n",
        "\n",
        "    def add_defense(self, card):\n",
        "        self.defenses.append((card.defend, card))\n",
        "        self.defenses.sort(key=lambda x: x[0])\n",
        "\n",
        "    def destroy_defense(self):\n",
        "        (_, card) = self.defenses.pop()\n",
        "        card.discard()\n",
        "\n",
        "    def take_damage(self, n):\n",
        "        if n == 0:\n",
        "            return\n",
        "        if len(self.defenses) == 0:\n",
        "            self.health = max(self.health-n, 0)\n",
        "            return\n",
        "        if self.defenses[0][0] > n:\n",
        "            self.defenses[0] = (self.defenses[0][0]-n, self.defenses[0][1])\n",
        "            return\n",
        "        (m, card) = self.defenses.pop(0)\n",
        "        card.discard()\n",
        "        self.take_damage(n-m)\n",
        "\n",
        "    def heal(self, n):\n",
        "        self.health = min(self.health+n, 10)\n",
        "\n",
        "    def play(self, n):\n",
        "        self.plays += n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZciNAS6xA9o"
      },
      "source": [
        "## game.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpTaxlDddd9m"
      },
      "outputs": [],
      "source": [
        "# from environments.dungeonmayhem.player import DungeonMayhem_Player\n",
        "# from environments.dungeonmayhem.character import DungeonMayhem_Barbarian, DungeonMayhem_Paladin, DungeonMayhem_Rogue, DungeonMayhem_Wizard\n",
        "\n",
        "class DungeonMayhem_Game:\n",
        "\n",
        "    def __init__(self, np_rng):\n",
        "        self.np_rng = np_rng\n",
        "        self.players = [DungeonMayhem_Player(i, self.np_rng) for i in range(4)]\n",
        "        self.characters = [DungeonMayhem_Barbarian, DungeonMayhem_Paladin, DungeonMayhem_Rogue, DungeonMayhem_Wizard]\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, char_perm=None, order_perm=None):\n",
        "        for player in self.players:\n",
        "            player.reset()\n",
        "        self.active_players = list(self.players)\n",
        "        self.immune_players = []\n",
        "        if order_perm is None:\n",
        "            order_perm = self.np_rng.permutation(4)\n",
        "        self.ordered_players = [self.players[i] for i in order_perm]\n",
        "        if char_perm is None:\n",
        "            char_perm = self.np_rng.permutation(4)\n",
        "        self.permuted_characters = [self.characters[i] for i in char_perm]\n",
        "        for player, character in zip(self.ordered_players, self.permuted_characters):\n",
        "            player.get_character_deck(character)\n",
        "            player.draw(3)\n",
        "        self.current_player = self.ordered_players[0]\n",
        "        self.current_player.start_turn()\n",
        "        self.winner = None\n",
        "        self.time_left = 200\n",
        "\n",
        "    def play_card(self, player, card):\n",
        "        player.hand.remove(card)\n",
        "        player.play(-1)\n",
        "\n",
        "        if card.power:\n",
        "            card.power[1](self, player, self.select_target(player))\n",
        "        if card.attack:\n",
        "            self.select_target(player).take_damage(card.attack)\n",
        "        if card.defend:\n",
        "            player.add_defense(card)\n",
        "        else:\n",
        "            card.discard()\n",
        "        if card.draw:\n",
        "            player.draw(card.draw)\n",
        "        if card.heal:\n",
        "            player.heal(card.heal)\n",
        "        if card.play:\n",
        "            player.play(card.play)\n",
        "\n",
        "        if len(player.hand) == 0:\n",
        "            player.draw(2)\n",
        "        for p in self.active_players:\n",
        "            if p.health == 0:\n",
        "                self.active_players.remove(p)\n",
        "                self.ordered_players.remove(p)\n",
        "                for (_, c) in p.defenses:\n",
        "                    c.discard()\n",
        "                p.deck = []\n",
        "                p.hand = []\n",
        "                p.discardpile = []\n",
        "                p.defenses = []\n",
        "\n",
        "        self.time_left -= 1\n",
        "        if len(self.ordered_players) == 0 or self.time_left == 0:\n",
        "            self.winner = DungeonMayhem_Player(4, self.np_rng)\n",
        "        elif len(self.ordered_players) == 1:\n",
        "            self.winner = self.ordered_players[0]\n",
        "        else:\n",
        "            if player == self.ordered_players[0] and player.plays == 0:\n",
        "                self.ordered_players.remove(player)\n",
        "                self.ordered_players.append(player)\n",
        "            self.current_player = self.ordered_players[0]\n",
        "            self.current_player.start_turn()\n",
        "            if self.current_player in self.immune_players:\n",
        "                self.immune_players.remove(self.current_player)\n",
        "                self.active_players.append(self.current_player)\n",
        "\n",
        "        # x = 0\n",
        "        # for p in self.players:\n",
        "        #     x += len(p.deck)\n",
        "        #     x += len(p.hand)\n",
        "        #     x += len(p.discardpile)\n",
        "        #     x += len(p.defenses)   \n",
        "        # print('')     \n",
        "        # print(x)\n",
        "        # print(player.character.character_id)\n",
        "        # print(target.character.character_id)\n",
        "        # print(card)\n",
        "        # print(card.player.character.character_id)\n",
        "        # for p in self.players:            \n",
        "        #     p_list = [\n",
        "        #         p.character.character_id,\n",
        "        #         p.health,\n",
        "        #         p.total_defenses(),\n",
        "        #         p.plays,\n",
        "        #         p in self.active_players,\n",
        "        #         p in self.immune_players,\n",
        "        #         p in self.ordered_players,\n",
        "        #         len(p.deck),\n",
        "        #         len(p.hand),\n",
        "        #         len(p.discardpile),\n",
        "        #         len(p.defenses),\n",
        "        #     ]\n",
        "        #     print(p_list)\n",
        "        # if x % 28 > 3 and x % 28 < 25:\n",
        "        #     raise Exception('FAULTY GAME MECHANIC')\n",
        "\n",
        "    def select_target(self, player):\n",
        "        sorted_players = sorted(self.players, key=lambda x: -x.total_health()+self.np_rng.random()/2)\n",
        "        for p in sorted_players:\n",
        "            if p != player and p not in self.immune_players:\n",
        "                return p\n",
        "\n",
        "    def get_game_state(self):\n",
        "        current_player = self.current_player\n",
        "        private_state = [\n",
        "            current_player.player_id,\n",
        "            current_player.character.character_id,\n",
        "            current_player.health,\n",
        "            current_player.total_defenses(),\n",
        "            int(current_player in self.active_players),\n",
        "            current_player.plays,\n",
        "            current_player.deck,\n",
        "            current_player.hand,\n",
        "        ]\n",
        "        public_states = [\n",
        "            [\n",
        "                player.health,\n",
        "                player.total_defenses(),\n",
        "                int(player in self.active_players),\n",
        "                player.discardpile,\n",
        "            ]\n",
        "            for player in sorted(self.players, key=lambda x: x.character.character_id)\n",
        "        ]\n",
        "        return private_state, public_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNq9xIhzxCRK"
      },
      "source": [
        "## environment.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbgLz-HngZIj"
      },
      "outputs": [],
      "source": [
        "from itertools import count, chain\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from environments.dungeonmayhem.player import DungeonMayhem_Game\n",
        "\n",
        "class DungeonMayhem_Environment:    \n",
        "\n",
        "    def __init__(self, device = None, random_seed = 6450, encoding_complexity = 0):\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "        self.np_rng = np.random.default_rng(random_seed)\n",
        "        self.game = DungeonMayhem_Game(self.np_rng)\n",
        "        self.num_actions, self.num_cards = self.prepare_cards(self.game)\n",
        "        self.encoding_complexity = encoding_complexity\n",
        "        self.state_dimensions = sum([\n",
        "            0, # current_player.player_id,\n",
        "            4, # current_player.character.character_id,\n",
        "            1, # current_player.health,\n",
        "            1, # current_player.total_defenses(),\n",
        "            1, # int(current_player in self.active_players),\n",
        "            1, # current_player.plays,\n",
        "            self.num_actions if self.encoding_complexity > 0 else 0, # current_player.deck,\n",
        "            self.num_actions, # current_player.hand,\n",
        "            4 * 1, # player.health,\n",
        "            4 * 1, # player.total_defenses(),\n",
        "            4 * 1, # int(player in self.active_players),\n",
        "            self.num_cards if self.encoding_complexity > 1 else 0, # player.discardpile,\n",
        "        ])\n",
        "        self.action_dimensions = 6\n",
        "        \n",
        "\n",
        "    def prepare_cards(self, game):\n",
        "        decks = [character.get_deck() for character in game.characters]\n",
        "\n",
        "        hash_tensor = torch.Tensor([1, 10, 100, 1000, 10000, 100000])\n",
        "        for deck in decks:\n",
        "            for card in deck:\n",
        "                card.action_encoding = torch.Tensor([\n",
        "                    card.attack,\n",
        "                    card.defend,\n",
        "                    card.draw,\n",
        "                    card.heal,\n",
        "                    card.play,\n",
        "                    0 if card.power is None else card.power[0],            \n",
        "                ])\n",
        "                card.action_id = torch.dot(card.action_encoding, hash_tensor).item()\n",
        "                card.action_encoding = card.action_encoding.to(device=self.device)\n",
        "\n",
        "        card_counter = count()\n",
        "        for deck in decks:\n",
        "            deck.sort(key=lambda x: x.action_id)\n",
        "            prev_card_id = -1\n",
        "            prev_action_id_hash = -1\n",
        "            for card in deck:\n",
        "                if card.action_id == prev_action_id_hash:\n",
        "                    card.card_id = prev_card_id\n",
        "                else:\n",
        "                    card.card_id = next(card_counter)\n",
        "                    prev_card_id = card.card_id\n",
        "                    prev_action_id_hash = card.action_id\n",
        "\n",
        "        action_counter = count()\n",
        "        cards = list(chain.from_iterable(decks))\n",
        "        cards.sort(key=lambda x: x.action_id)\n",
        "        prev_action_id = -1\n",
        "        prev_action_id_hash = -1\n",
        "        for card in cards:\n",
        "            if card.action_id == prev_action_id_hash:\n",
        "                card.action_id = prev_action_id\n",
        "            else:\n",
        "                prev_action_id_hash = card.action_id\n",
        "                card.action_id = next(action_counter)\n",
        "                prev_action_id = card.action_id\n",
        "\n",
        "        return next(action_counter), next(card_counter)\n",
        "\n",
        "    def reset(self):\n",
        "        self.game.reset()\n",
        "\n",
        "    def get_winner(self):\n",
        "        return self.game.winner\n",
        "\n",
        "    def get_current_player_details(self):\n",
        "        game_state = self.game.get_game_state()\n",
        "        id = game_state[0][0] # current_player.player_id,\n",
        "\n",
        "        state = torch.zeros(self.state_dimensions)\n",
        "        idx = 0\n",
        "        state[idx + game_state[0][1]] = 1 # current_player.character.character_id,\n",
        "        idx += 4\n",
        "        state[idx] = game_state[0][2] # current_player.health,\n",
        "        idx += 1\n",
        "        state[idx] = game_state[0][3] # current_player.total_defenses(),\n",
        "        idx += 1\n",
        "        state[idx] = game_state[0][4] # int(current_player in self.active_players),\n",
        "        idx += 1\n",
        "        state[idx] = game_state[0][5] # current_player.plays,\n",
        "        idx += 1\n",
        "        if self.encoding_complexity > 0:\n",
        "            for card in game_state[0][6]:\n",
        "                state[idx + card.action_id] += 1 # current_player.deck\n",
        "            idx += self.num_actions\n",
        "        for card in game_state[0][7]:\n",
        "            state[idx + card.action_id] += 1 # current_player.hand\n",
        "        idx += self.num_actions\n",
        "        for i in range(4):\n",
        "            state[idx + i] = game_state[1][i][0] # player.health,\n",
        "        idx += 4\n",
        "        for i in range(4):\n",
        "            state[idx + i] = game_state[1][i][1] # player.total_defenses(),\n",
        "        idx += 4\n",
        "        for i in range(4):\n",
        "            state[idx + i] = game_state[1][i][2] # int(player in self.active_players),\n",
        "        idx += 4\n",
        "        if self.encoding_complexity > 1:\n",
        "            for i in range(4):\n",
        "                for card in game_state[1][i][3]:\n",
        "                    state[idx + card.card_id] += 1 # player.discardpile,\n",
        "        state = state.to(device=self.device)\n",
        "\n",
        "        actions = torch.vstack([\n",
        "            card.action_encoding\n",
        "            for card in game_state[0][7]\n",
        "        ])\n",
        "\n",
        "        return id, state, actions\n",
        "\n",
        "    def apply_action(self, idx):\n",
        "        player = self.game.current_player\n",
        "        card = self.game.current_player.hand[idx]\n",
        "        self.game.play_card(player, card)\n",
        "\n",
        "    def get_health_defense_idxs(self):\n",
        "        idx = sum([\n",
        "            0,\n",
        "            4,\n",
        "            1,\n",
        "            1,\n",
        "            1,\n",
        "            1,\n",
        "            self.num_actions if self.encoding_complexity > 0 else 0,\n",
        "            self.num_actions,\n",
        "        ])\n",
        "        return (4, 5), [(idx+i, idx+4+i) for i in range(4)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEcgxB-g4OUi"
      },
      "source": [
        "# agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cY3pw5qHX1_"
      },
      "source": [
        "## random_agent.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YJHaG4GHXXL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import count\n",
        "\n",
        "class Random_Agent:\n",
        "    agent_counter = count()\n",
        "\n",
        "    def __init__(self, random_seed = 6450):\n",
        "        self.np_rng = np.random.default_rng(random_seed + next(Random_Agent.agent_counter))\n",
        "\n",
        "    def step(self, state_actions):\n",
        "        return self.np_rng.integers(len(state_actions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNQPnsdi5TdD"
      },
      "source": [
        "## nn_estimator.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvKn3uQZ5UvF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from itertools import count\n",
        "\n",
        "class NN_Model(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        hidden_dims,\n",
        "    ):\n",
        "        super(NN_Model, self).__init__()\n",
        "        dims = [input_dim] + hidden_dims\n",
        "        fc = []\n",
        "        for i in range(len(dims)-1):\n",
        "            fc.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            fc.append(nn.LeakyReLU())\n",
        "        fc.append(nn.Linear(dims[-1], 1))\n",
        "        fc.append(nn.Tanh())\n",
        "        self.fc_layers = nn.Sequential(*fc)\n",
        "\n",
        "    def forward(self, sa):\n",
        "        q_sa = self.fc_layers(sa).squeeze(1)\n",
        "        return q_sa\n",
        "\n",
        "class NN_Estimator:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        hidden_dims,\n",
        "        learning_rate,\n",
        "        device = None,\n",
        "        name = '',\n",
        "    ):\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        self.model = NN_Model(input_dim, hidden_dims).to(self.device)\n",
        "        for p in self.model.parameters():\n",
        "            if len(p.data.shape) > 1:\n",
        "                nn.init.xavier_uniform_(p.data)\n",
        "\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        self.name = name\n",
        "        self.save_counter = count()\n",
        "\n",
        "    def predict(self, sa):\n",
        "        with torch.no_grad():\n",
        "            q_sa = self.model(sa)\n",
        "        return q_sa\n",
        "\n",
        "    def train(self, sa, y):\n",
        "        self.optimizer.zero_grad()\n",
        "        q_sa = self.model(sa)\n",
        "        loss = self.criterion(q_sa, y)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def save_model(self):\n",
        "        save_dir = '{}_{}.pt'.format(self.name, next(self.save_counter))\n",
        "        torch.save(self.model.state_dict(), save_dir)\n",
        "\n",
        "    def load_model(self, load_dir):\n",
        "        self.model.load_state_dict(torch.load(load_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8ArQ4Dd4QES"
      },
      "source": [
        "## dqn_agent.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh5lxXkJitJ5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import count\n",
        "from copy import deepcopy\n",
        "import logging\n",
        "\n",
        "# from agents.nn_estimator import NN_Estimator\n",
        "\n",
        "class DQN_Agent:\n",
        "    agent_counter = count()\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_action_dims,\n",
        "        learning_rate=0.0001,\n",
        "        exploration_rate=0.1,\n",
        "        batch_size=1024,\n",
        "        replay_memory_max_size=100000,\n",
        "        replay_memory_min_size=10000,\n",
        "        train_learner_estimator_every=100,\n",
        "        update_target_estimator_every=100,\n",
        "        hidden_dims=[512]*3,\n",
        "        device=None,\n",
        "        name='',\n",
        "        random_seed=6450,\n",
        "    ):\n",
        "\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.replay_memory_min_size = replay_memory_min_size\n",
        "        self.replay_memory_max_size = replay_memory_max_size\n",
        "        self.train_learner_estimator_every = train_learner_estimator_every\n",
        "        self.update_target_estimator_every = update_target_estimator_every\n",
        "\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        self.total_t = 0\n",
        "        self.train_t = 0\n",
        "        self.memory = []\n",
        "\n",
        "        self.learner_estimator = NN_Estimator(\n",
        "            input_dim=state_action_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            learning_rate=learning_rate,\n",
        "            device=self.device,\n",
        "            name='{}_dqn_learner'.format(name),\n",
        "        )\n",
        "        self.target_estimator = NN_Estimator(\n",
        "            input_dim=state_action_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            learning_rate=learning_rate,\n",
        "            device=self.device,\n",
        "            name='{}_dqn_target'.format(name),\n",
        "        )\n",
        "\n",
        "        self.np_rng = np.random.default_rng(random_seed + next(DQN_Agent.agent_counter))\n",
        "\n",
        "    def step(self, state_actions):\n",
        "        if self.np_rng.random() < self.exploration_rate:\n",
        "            return self.np_rng.integers(len(state_actions))\n",
        "        return self.learner_estimator.predict(state_actions).argmax().item()\n",
        "\n",
        "    def feed(self, state_action, reward, next_state_actions):\n",
        "        if len(self.memory) == self.replay_memory_max_size:\n",
        "            self.memory.pop(0)\n",
        "        self.memory.append([state_action, reward, next_state_actions])\n",
        "        self.total_t += 1\n",
        "        tmp = self.total_t - self.replay_memory_min_size\n",
        "        if tmp >= 0 and tmp % self.train_learner_estimator_every == 0:\n",
        "            return self.train()\n",
        "        return 0\n",
        "\n",
        "    def train(self):\n",
        "        state_action_list = []\n",
        "        reward_list = []\n",
        "        next_state_actions_list = []\n",
        "        idxs = self.np_rng.integers(len(self.memory)-self.batch_size+1, size=self.batch_size)\n",
        "        for i, idx in enumerate(idxs):\n",
        "            x = self.memory[i+idx]\n",
        "            state_action_list.append(x[0])\n",
        "            reward_list.append(x[1])\n",
        "            next_state_actions_list.append(x[2])\n",
        "        state_action_batch = torch.stack(state_action_list)\n",
        "        reward_batch = torch.Tensor(reward_list).to(self.device)\n",
        "        next_state_actions_batch = torch.cat(next_state_actions_list)\n",
        "        next_state_actions_sizes = [len(x) for x in next_state_actions_list]\n",
        "\n",
        "        q_values_next_learner = self.learner_estimator.predict(next_state_actions_batch)\n",
        "        q_values_next_learner_split = torch.split(q_values_next_learner, next_state_actions_sizes)\n",
        "        best_state_actions_list = [sa_s[q_sa_s.argmax()] for sa_s, q_sa_s in zip(next_state_actions_list, q_values_next_learner_split)]\n",
        "        best_state_actions_batch = torch.stack(best_state_actions_list)\n",
        "        q_values_next_target = self.target_estimator.predict(best_state_actions_batch)\n",
        "        target_batch = reward_batch + reward_batch.eq(0).float() * q_values_next_target\n",
        "\n",
        "        loss = self.learner_estimator.train(state_action_batch, target_batch)\n",
        "        # print(\"\\rStep: {} Loss: {}\".format(self.total_t, loss), end=\"\")\n",
        "\n",
        "        if self.train_t % self.update_target_estimator_every == 0:\n",
        "            logging.info('Step: {} Loss: {}'.format(self.total_t, loss))\n",
        "            self.target_estimator = deepcopy(self.learner_estimator)\n",
        "            logging.info('Target Updated')\n",
        "            #self.learner_estimator.save_model()            \n",
        "\n",
        "        self.train_t += 1\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dmc_agent.py"
      ],
      "metadata": {
        "id": "y2n094JYZ-dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import count\n",
        "import logging\n",
        "\n",
        "# from agents.nn_estimator import NN_Estimator\n",
        "\n",
        "class DMC_Agent:\n",
        "    agent_counter = count()\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_action_dims,\n",
        "        learning_rate=0.0001,\n",
        "        exploration_rate=0.1,\n",
        "        train_estimator_every=10,\n",
        "        hidden_dims=[512]*3,\n",
        "        device=None,\n",
        "        name='',\n",
        "        random_seed=6450,\n",
        "    ):\n",
        "\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.train_estimator_every = train_estimator_every\n",
        "\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        self.t = 0\n",
        "        self.memory = []\n",
        "\n",
        "        self.estimator = NN_Estimator(\n",
        "            input_dim=state_action_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            learning_rate=learning_rate,\n",
        "            device=self.device,\n",
        "            name='{}_dmc_estimator'.format(name),\n",
        "        )\n",
        "\n",
        "        self.np_rng = np.random.default_rng(random_seed + next(DMC_Agent.agent_counter))\n",
        "\n",
        "    def step(self, state_actions):\n",
        "        if self.np_rng.random() < self.exploration_rate:\n",
        "            return self.np_rng.integers(len(state_actions))\n",
        "        return self.estimator.predict(state_actions).argmax().item()\n",
        "\n",
        "    def feed(self, episodes):\n",
        "        for state_actions, reward in episodes:\n",
        "            T = len(state_actions)\n",
        "            if T == 0:\n",
        "                continue\n",
        "            state_actions = torch.stack(state_actions)\n",
        "            reward = reward * torch.ones(T).to(self.device)\n",
        "            self.memory.append((state_actions, reward))\n",
        "        self.t += 1\n",
        "        if self.t % self.train_estimator_every == 0:\n",
        "            return self.train()\n",
        "        return 0\n",
        "\n",
        "    def train(self):\n",
        "        state_actions_batch = torch.cat([x[0] for x in self.memory])\n",
        "        reward_batch = torch.cat([x[1] for x in self.memory])\n",
        "        loss = self.estimator.train(state_actions_batch, reward_batch)\n",
        "        logging.info('Step: {} Loss: {}'.format(self.t, loss))\n",
        "        self.memory = []\n",
        "        return loss"
      ],
      "metadata": {
        "id": "w_VXLWVoS-gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu_MVfXt4jwk"
      },
      "source": [
        "# Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from math import exp\n",
        "import logging\n",
        "import pickle\n",
        "\n",
        "# from environments.dungeonmayhem.environment import DungeonMayhem_Environment\n",
        "# from agents.nn_estimator import NN_Estimator\n",
        "\n",
        "def pretrain(num_episodes = 100000, num_epochs = 100, batch_size = 1024, name=''):\n",
        "\n",
        "    log = logging.getLogger()\n",
        "    for hdlr in log.handlers[:]:\n",
        "        if isinstance(hdlr, logging.FileHandler):\n",
        "            log.removeHandler(hdlr)\n",
        "    filehandler = logging.FileHandler('{}_pretraining.log'.format(name), 'w')\n",
        "    log.addHandler(filehandler)\n",
        "\n",
        "    environment = DungeonMayhem_Environment()\n",
        "    player_idxs, game_idxs = environment.get_health_defense_idxs()\n",
        "    dims = environment.state_dimensions + environment.action_dimensions\n",
        "    estimator = NN_Estimator(dims, hidden_dims=[512]*3, learning_rate=0.0001, name='{}_pretrained'.format(name))\n",
        "    memory = []\n",
        "    loss_list = []\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    np_rng = np.random.default_rng(6450)\n",
        "\n",
        "    for episode_id in range(1, 1+num_episodes):\n",
        "        environment.reset()\n",
        "        prev_state_actions = [None, None, None, None]        \n",
        "\n",
        "        while environment.get_winner() is None:\n",
        "            id, state, actions = environment.get_current_player_details()\n",
        "            if prev_state_actions[id] is not None:\n",
        "                numerator = exp(state[player_idxs[0]] + state[player_idxs[1]])\n",
        "                denominator = sum([exp(state[y[0]] + state[y[1]]) for y in game_idxs])\n",
        "                y = numerator / denominator\n",
        "                memory.append((prev_state_actions[id][0], prev_state_actions[id][1], 2*y-1))\n",
        "            action_idx = np_rng.integers(len(actions))\n",
        "            prev_state_actions[id] = (state, actions[action_idx])\n",
        "            environment.apply_action(action_idx)\n",
        "\n",
        "        winner_id = environment.get_winner().player_id\n",
        "        for id in range(4):\n",
        "            state_action = prev_state_actions[id]\n",
        "            if state_action is None:\n",
        "                continue\n",
        "            reward = 1 if id == winner_id else -1            \n",
        "            memory.append((state_action[0], state_action[1], reward))            \n",
        "\n",
        "        if episode_id % 1000 == 0:\n",
        "            logging.info('Episode: {}/{} DONE!'.format(episode_id, num_episodes))\n",
        "\n",
        "    def process_memory_entry(x):\n",
        "        return torch.cat((x[0], x[1])), x[2]\n",
        "    for i, x in enumerate(memory):\n",
        "        memory[i] = process_memory_entry(memory[i])\n",
        "\n",
        "    return estimator, memory\n",
        "\n",
        "    for epoch_id in range(1, 1+num_epochs):\n",
        "        iii = np.array_split(np_rng.permutation(len(memory)), len(memory)//batch_size+1)\n",
        "        for ii in iii:\n",
        "            state_action_batch = torch.stack([memory[i][0] for i in ii])\n",
        "            target_batch = torch.Tensor([memory[i][1] for i in ii]).to(device)\n",
        "            loss = estimator.train(state_action_batch, target_batch)\n",
        "            if loss > 0:\n",
        "                loss_list.append(loss)\n",
        "        logging.info('Loss: {}'.format(loss))\n",
        "        logging.info('Epoch: {}/{} DONE!'.format(epoch_id, num_epochs))\n",
        "\n",
        "        if epoch_id == num_epochs // 2 or epoch_id == num_epochs:\n",
        "            estimator.save_model()\n",
        "    with open('{}_pretraining_loss_list'.format(name), 'wb') as fp:\n",
        "        pickle.dump(loss_list, fp)\n",
        "\n",
        "    return estimator, loss_list"
      ],
      "metadata": {
        "id": "CQtYanHcD_A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7YxdTi2f9wFr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5uftg4jNcDV"
      },
      "outputs": [],
      "source": [
        "def round(environment, agents):\n",
        "    environment.reset()\n",
        "    prev_state_actions = [None, None, None, None]        \n",
        "    if not isinstance(agents, list):\n",
        "        agents = [agents] * 4\n",
        "\n",
        "    while environment.get_winner() is None:\n",
        "        id, state, actions = environment.get_current_player_details()            \n",
        "        state = state.expand(len(actions), -1)\n",
        "        state_actions = torch.cat((state, actions), dim=1)\n",
        "        action_idx = agents[id].step(state_actions)\n",
        "        prev_state_actions[id] = state_actions[action_idx]\n",
        "        environment.apply_action(action_idx)\n",
        "\n",
        "    winner_id = environment.get_winner().player_id\n",
        "    return [1 if id == winner_id else -1 for id in range(4)]\n",
        "\n",
        "def tournament(environment, agents, num_games):\n",
        "    payoffs = [0 for _ in range(4)]\n",
        "    for _ in range(num_games):\n",
        "        _payoffs = round(environment, agents)\n",
        "        for i, _ in enumerate(payoffs):\n",
        "            payoffs[i] += _payoffs[i]\n",
        "    for i, _ in enumerate(payoffs):\n",
        "        payoffs[i] /= num_games\n",
        "    return payoffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlXCD2O04jcB"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import pickle\n",
        "# from agents.dqn_agent import DQN_Agent\n",
        "# from environments.dungeonmayhem.environment import DungeonMayhem_Environment\n",
        "\n",
        "def train_DQN(\n",
        "    hidden_dims = [512]*3,\n",
        "    encoding_complexity = 0,\n",
        "    pretraining_file_name = None,\n",
        "    num_episodes = 100000,\n",
        "    random_seed = 6450,\n",
        "    name='',\n",
        "):\n",
        "\n",
        "    log = logging.getLogger()\n",
        "    for hdlr in log.handlers[:]:\n",
        "        if isinstance(hdlr, logging.FileHandler):\n",
        "            log.removeHandler(hdlr)\n",
        "    filehandler = logging.FileHandler('{}_dqn_training.log'.format(name), 'w')\n",
        "    log.addHandler(filehandler)\n",
        "\n",
        "    environment = DungeonMayhem_Environment(encoding_complexity = encoding_complexity, random_seed = random_seed)\n",
        "    state_action_dims = environment.state_dimensions + environment.action_dimensions\n",
        "    agent = DQN_Agent(state_action_dims, hidden_dims = hidden_dims, random_seed = random_seed, name=name)\n",
        "\n",
        "    if pretraining_file_name is not None:\n",
        "        agent.learner_estimator.load_model(pretraining_file_name)\n",
        "        agent.target_estimator.load_model(pretraining_file_name)\n",
        "\n",
        "    loss_list = []\n",
        "    payoffs_list = []\n",
        "\n",
        "    for episode_id in range(1, 1+num_episodes):\n",
        "        environment.reset()\n",
        "        prev_state_actions = [None, None, None, None]        \n",
        "\n",
        "        while environment.get_winner() is None:\n",
        "            id, state, actions = environment.get_current_player_details()            \n",
        "            state = state.expand(len(actions), -1)\n",
        "            state_actions = torch.cat((state, actions), dim=1)\n",
        "            if prev_state_actions[id] is not None:\n",
        "                loss = agent.feed(prev_state_actions[id], 0, state_actions)\n",
        "                if loss > 0:\n",
        "                    loss_list.append(loss)\n",
        "            action_idx = agent.step(state_actions)\n",
        "            prev_state_actions[id] = state_actions[action_idx]\n",
        "            environment.apply_action(action_idx)\n",
        "\n",
        "        winner_id = environment.get_winner().player_id\n",
        "        for id in range(4):\n",
        "            state_action = prev_state_actions[id]\n",
        "            if state_action is None:\n",
        "                continue\n",
        "            reward = 1 if id == winner_id else -1            \n",
        "            terminal = torch.zeros_like(state_action).unsqueeze(dim=0)\n",
        "            loss = agent.feed(state_action, reward, terminal)\n",
        "            if loss > 0:\n",
        "                loss_list.append(loss)\n",
        "\n",
        "        if episode_id % 100 == 0:\n",
        "            logging.info('Episode: {}/{} DONE!'.format(episode_id, num_episodes))\n",
        "            exp = agent.exploration_rate\n",
        "            agent.exploration_rate = 0\n",
        "            payoffs = tournament(environment, [agent]+[Random_Agent()]*3, 100)\n",
        "            agent.exploration_rate = exp\n",
        "            payoffs_list.append(payoffs)\n",
        "            logging.info('Payoffs: {}'.format(payoffs))\n",
        "\n",
        "        if episode_id % 1000 == 0:\n",
        "            agent.learner_estimator.save_model()\n",
        "            logging.info('Estimator Saved')\n",
        "\n",
        "    with open('{}_dqn_training_loss_list'.format(name), 'wb') as fp:\n",
        "        pickle.dump(loss_list, fp)\n",
        "    with open('{}_dqn_training_payoffs_list'.format(name), 'wb') as fp:\n",
        "        pickle.dump(payoffs_list, fp)\n",
        "\n",
        "    return agent, loss_list, payoffs_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pickle\n",
        "# from agents.dmc_agent import DMC_Agent\n",
        "# from environments.dungeonmayhem.environment import DungeonMayhem_Environment\n",
        "\n",
        "def train_DMC(\n",
        "    hidden_dims = [512]*3,\n",
        "    encoding_complexity = 0,\n",
        "    pretraining_file_name = None,\n",
        "    num_episodes = 200000,\n",
        "    random_seed = 6450,\n",
        "    name='',\n",
        "):\n",
        "\n",
        "    log = logging.getLogger()\n",
        "    for hdlr in log.handlers[:]:\n",
        "        if isinstance(hdlr, logging.FileHandler):\n",
        "            log.removeHandler(hdlr)\n",
        "    filehandler = logging.FileHandler('{}_dmc_training.log'.format(name), 'w')\n",
        "    log.addHandler(filehandler)\n",
        "\n",
        "    environment = DungeonMayhem_Environment(encoding_complexity = encoding_complexity, random_seed = random_seed)\n",
        "    state_action_dims = environment.state_dimensions + environment.action_dimensions\n",
        "    agent = DMC_Agent(state_action_dims, hidden_dims = hidden_dims, random_seed = random_seed, name=name)\n",
        "\n",
        "    if pretraining_file_name is not None:\n",
        "        agent.estimator.load_model(pretraining_file_name)\n",
        "\n",
        "    loss_list = []\n",
        "    payoffs_list = []\n",
        "\n",
        "    for episode_id in range(1, 1+num_episodes):\n",
        "        environment.reset()\n",
        "        prev_state_actions = [[], [], [], []]        \n",
        "\n",
        "        while environment.get_winner() is None:\n",
        "            id, state, actions = environment.get_current_player_details()            \n",
        "            state = state.expand(len(actions), -1)\n",
        "            state_actions = torch.cat((state, actions), dim=1)\n",
        "            action_idx = agent.step(state_actions)\n",
        "            prev_state_actions[id].append(state_actions[action_idx])\n",
        "            environment.apply_action(action_idx)\n",
        "\n",
        "        winner_id = environment.get_winner().player_id\n",
        "        episodes = []\n",
        "        for id in range(4):\n",
        "            state_action = prev_state_actions[id]\n",
        "            reward = 1 if id == winner_id else -1\n",
        "            episodes.append((state_action, reward))         \n",
        "        \n",
        "        loss = agent.feed(episodes)\n",
        "        if loss > 0:\n",
        "            loss_list.append(loss)\n",
        "\n",
        "        if episode_id % 100 == 0:\n",
        "            logging.info('Episode: {}/{} DONE!'.format(episode_id, num_episodes))\n",
        "            exp = agent.exploration_rate\n",
        "            agent.exploration_rate = 0\n",
        "            payoffs = tournament(environment, [agent]+[Random_Agent()]*3, 100)\n",
        "            agent.exploration_rate = exp\n",
        "            payoffs_list.append(payoffs)\n",
        "            logging.info('Payoffs: {}'.format(payoffs))\n",
        "\n",
        "        if episode_id % 1000 == 0:\n",
        "            agent.estimator.save_model()\n",
        "            logging.info('Estimator Saved')\n",
        "\n",
        "    with open('{}_dmc_training_loss_list'.format(name), 'wb') as fp:\n",
        "        pickle.dump(loss_list, fp)\n",
        "    with open('{}_dmc_training_payoffs_list'.format(name), 'wb') as fp:\n",
        "        pickle.dump(payoffs_list, fp)\n",
        "\n",
        "    return agent, loss_list, payoffs_list"
      ],
      "metadata": {
        "id": "bgzy_UN9cRk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "4MYx3kLQ3nNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # For Connecting to Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG-zw5wJ4-dI",
        "outputId": "28019f00-a9bc-44ed-fbb6-3b70f19cde8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # For Creating Folder 'For Albert'\n",
        "# folder_path = '/content/drive/MyDrive/For Albert'\n",
        "# import os\n",
        "# if not os.path.exists(folder_path):\n",
        "#     os.makedirs(folder_path)"
      ],
      "metadata": {
        "id": "SIeeYMZa5Xz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Showing Logs on Screen\n",
        "import logging\n",
        "log = logging.getLogger()\n",
        "log.addHandler(logging.StreamHandler())\n",
        "log.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "BB5eyO3o-An0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Pretraining\n",
        "# estimator, loss_list = pretrain(\n",
        "#     name=folder_path+'/RL',\n",
        "# )"
      ],
      "metadata": {
        "id": "DzOtyfAsf8cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKBbabwg7vph"
      },
      "outputs": [],
      "source": [
        "# For Training Model A\n",
        "# agent, loss_list, payoffs_list = train_DQN(\n",
        "#     name=folder_path+'/50E_pretrained',\n",
        "#     pretraining_file_name=folder_path+'/RL_pretrained_0.pt',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # For Training Model B\n",
        "# agent, loss_list, payoffs_list = train_DQN(\n",
        "#     name=folder_path+'/100E_pretrained',\n",
        "#     pretraining_file_name=folder_path+'/RL_pretrained_1.pt',\n",
        "# )"
      ],
      "metadata": {
        "id": "kPRK79dS6mBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # For Training Model C\n",
        "# agent, loss_list, payoffs_list = train_DMC(\n",
        "#     name=folder_path+'/50E_pretrained',\n",
        "#     pretraining_file_name=folder_path+'/RL_pretrained_0.pt',\n",
        "# )"
      ],
      "metadata": {
        "id": "87Cmvu7Qe6vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # For Training Model D\n",
        "# agent, loss_list, payoffs_list = train_DMC(\n",
        "#     name=folder_path+'/100E_pretrained',\n",
        "#     pretraining_file_name=folder_path+'/RL_pretrained_1.pt',\n",
        "# )"
      ],
      "metadata": {
        "id": "e2aM1fwYmQLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import multiprocessing\n",
        "\n",
        "# def train_DQN_multiprocessing_wrapper(q, i):\n",
        "#     if i == 0:\n",
        "#         agent, loss_list, payoffs_list = train_DQN(\n",
        "#             name=folder_path+'/50E_pretrained',\n",
        "#             pretraining_file_name=folder_path+'/RL_pretrained_0.pt',\n",
        "#         )        \n",
        "#     else:\n",
        "#         agent, loss_list, payoffs_list = train_DQN(\n",
        "#             name=folder_path+'/100E_pretrained',\n",
        "#             pretraining_file_name=folder_path+'/RL_pretrained_1.pt',\n",
        "#         )\n",
        "#     q.put(('agent_{}'.format(i), agent))\n",
        "#     q.put(('loss_list_{}'.format(i), loss_list))\n",
        "#     q.put(('payoffs_list_{}'.format(i), payoffs_list))"
      ],
      "metadata": {
        "id": "k19XPc6dCwwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# q = multiprocessing.Queue()\n",
        "# p0 = multiprocessing.Process(target=train_DQN_multiprocessing_wrapper, args=(q,0))\n",
        "# p1 = multiprocessing.Process(target=train_DQN_multiprocessing_wrapper, args=(q,1))\n",
        "# p0.start()\n",
        "# p1.start()\n",
        "# p0.join()\n",
        "# p1.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qSlxqTsoFCuA",
        "outputId": "3084afe6-2225-4e56-e6ac-0195a6cac6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode: 100/100000 DONE!\n",
            "Episode: 100/100000 DONE!\n",
            "Payoffs: [-0.12, -0.58, -0.62, -0.68]\n",
            "Payoffs: [-0.22, -0.58, -0.56, -0.64]\n",
            "Step: 10000 Loss: 0.16688847541809082\n",
            "Target Updated\n",
            "Step: 10000 Loss: 0.14960427582263947\n",
            "Target Updated\n",
            "Episode: 200/100000 DONE!\n",
            "Episode: 200/100000 DONE!\n",
            "Payoffs: [-0.14, -0.8, -0.4, -0.66]\n",
            "Payoffs: [0.16, -0.8, -0.7, -0.68]\n",
            "Step: 20000 Loss: 0.09804263710975647\n",
            "Target Updated\n",
            "Step: 20000 Loss: 0.09436871856451035\n",
            "Target Updated\n",
            "Episode: 300/100000 DONE!\n",
            "Episode: 300/100000 DONE!\n",
            "Payoffs: [-0.14, -0.68, -0.62, -0.56]\n",
            "Payoffs: [-0.14, -0.68, -0.7, -0.5]\n",
            "Step: 30000 Loss: 0.07889556884765625\n",
            "Target Updated\n",
            "Step: 30000 Loss: 0.06730889528989792\n",
            "Target Updated\n",
            "Episode: 400/100000 DONE!\n",
            "Payoffs: [-0.24, -0.46, -0.56, -0.74]\n",
            "Episode: 400/100000 DONE!\n",
            "Payoffs: [0.1, -0.62, -0.7, -0.78]\n",
            "Step: 40000 Loss: 0.06561850011348724\n",
            "Target Updated\n",
            "Step: 40000 Loss: 0.048055216670036316\n",
            "Target Updated\n",
            "Episode: 500/100000 DONE!\n",
            "Payoffs: [-0.12, -0.8, -0.54, -0.56]\n",
            "Episode: 500/100000 DONE!\n",
            "Payoffs: [-0.08, -0.5, -0.64, -0.78]\n",
            "Episode: 600/100000 DONE!\n",
            "Payoffs: [-0.24, -0.68, -0.56, -0.52]\n",
            "Step: 50000 Loss: 0.040136538445949554\n",
            "Target Updated\n",
            "Step: 50000 Loss: 0.04795307666063309\n",
            "Target Updated\n",
            "Episode: 600/100000 DONE!\n",
            "Payoffs: [-0.02, -0.76, -0.62, -0.64]\n",
            "Episode: 700/100000 DONE!\n",
            "Payoffs: [-0.08, -0.72, -0.6, -0.62]\n",
            "Step: 60000 Loss: 0.038281649351119995\n",
            "Target Updated\n",
            "Episode: 700/100000 DONE!\n",
            "Step: 60000 Loss: 0.0431828536093235\n",
            "Target Updated\n",
            "Payoffs: [-0.22, -0.68, -0.52, -0.58]\n",
            "Episode: 800/100000 DONE!\n",
            "Payoffs: [-0.08, -0.72, -0.56, -0.66]\n",
            "Step: 70000 Loss: 0.035429634153842926\n",
            "Target Updated\n",
            "Episode: 800/100000 DONE!\n",
            "Step: 70000 Loss: 0.049459733068943024\n",
            "Target Updated\n",
            "Payoffs: [-0.06, -0.62, -0.66, -0.66]\n",
            "Episode: 900/100000 DONE!\n",
            "Payoffs: [-0.24, -0.66, -0.6, -0.52]\n",
            "Episode: 900/100000 DONE!\n",
            "Payoffs: [-0.12, -0.7, -0.48, -0.7]\n",
            "Episode: 1000/100000 DONE!\n",
            "Step: 80000 Loss: 0.03600919991731644\n",
            "Target Updated\n",
            "Payoffs: [-0.04, -0.64, -0.6, -0.72]\n",
            "Estimator Saved\n",
            "Step: 80000 Loss: 0.045712996274232864\n",
            "Target Updated\n",
            "Episode: 1000/100000 DONE!\n",
            "Payoffs: [0.02, -0.6, -0.66, -0.76]\n",
            "Estimator Saved\n",
            "Episode: 1100/100000 DONE!\n",
            "Payoffs: [-0.18, -0.6, -0.54, -0.68]\n",
            "Step: 90000 Loss: 0.031890399754047394\n",
            "Target Updated\n",
            "Step: 90000 Loss: 0.05957270413637161\n",
            "Target Updated\n",
            "Episode: 1100/100000 DONE!\n",
            "Episode: 1200/100000 DONE!\n",
            "Payoffs: [-0.12, -0.6, -0.64, -0.64]\n",
            "Payoffs: [-0.12, -0.72, -0.66, -0.52]\n",
            "Step: 100000 Loss: 0.03656536340713501\n",
            "Target Updated\n",
            "Step: 100000 Loss: 0.043826133012771606\n",
            "Target Updated\n",
            "Episode: 1200/100000 DONE!\n",
            "Episode: 1300/100000 DONE!\n",
            "Payoffs: [-0.02, -0.6, -0.78, -0.64]\n",
            "Payoffs: [-0.06, -0.72, -0.56, -0.68]\n",
            "Step: 110000 Loss: 0.0401722714304924\n",
            "Target Updated\n",
            "Step: 110000 Loss: 0.039251942187547684\n",
            "Target Updated\n",
            "Episode: 1300/100000 DONE!\n",
            "Payoffs: [0.02, -0.7, -0.76, -0.56]\n",
            "Episode: 1400/100000 DONE!\n",
            "Payoffs: [-0.06, -0.68, -0.76, -0.5]\n",
            "Episode: 1400/100000 DONE!\n",
            "Step: 120000 Loss: 0.04132480174303055\n",
            "Target Updated\n",
            "Payoffs: [-0.16, -0.56, -0.62, -0.68]\n",
            "Step: 120000 Loss: 0.02505462057888508\n",
            "Target Updated\n",
            "Episode: 1500/100000 DONE!\n",
            "Payoffs: [0.1, -0.7, -0.72, -0.68]\n",
            "Episode: 1500/100000 DONE!\n",
            "Payoffs: [-0.04, -0.62, -0.64, -0.72]\n",
            "Step: 130000 Loss: 0.034188300371170044\n",
            "Target Updated\n",
            "Episode: 1600/100000 DONE!\n",
            "Step: 130000 Loss: 0.04566320404410362\n",
            "Target Updated\n",
            "Payoffs: [-0.28, -0.62, -0.62, -0.52]\n",
            "Episode: 1600/100000 DONE!\n",
            "Payoffs: [-0.12, -0.54, -0.64, -0.7]\n",
            "Step: 140000 Loss: 0.038973767310380936\n",
            "Target Updated\n",
            "Episode: 1700/100000 DONE!\n",
            "Step: 140000 Loss: 0.03432433307170868\n",
            "Target Updated\n",
            "Payoffs: [-0.12, -0.62, -0.72, -0.54]\n",
            "Episode: 1700/100000 DONE!\n",
            "Payoffs: [-0.12, -0.58, -0.68, -0.62]\n",
            "Episode: 1800/100000 DONE!\n",
            "Step: 150000 Loss: 0.032752227038145065\n",
            "Target Updated\n",
            "Payoffs: [-0.12, -0.62, -0.6, -0.68]\n",
            "Step: 150000 Loss: 0.0451510027050972\n",
            "Target Updated\n",
            "Episode: 1800/100000 DONE!\n",
            "Payoffs: [0.0, -0.62, -0.6, -0.78]\n",
            "Episode: 1900/100000 DONE!\n",
            "Step: 160000 Loss: 0.04546322673559189\n",
            "Target Updated\n",
            "Payoffs: [-0.1, -0.68, -0.64, -0.58]\n",
            "Step: 160000 Loss: 0.04635198414325714\n",
            "Target Updated\n",
            "Episode: 1900/100000 DONE!\n",
            "Payoffs: [0.0, -0.6, -0.66, -0.76]\n",
            "Episode: 2000/100000 DONE!\n",
            "Step: 170000 Loss: 0.033722881227731705\n",
            "Target Updated\n",
            "Payoffs: [-0.18, -0.64, -0.62, -0.6]\n",
            "Estimator Saved\n",
            "Step: 170000 Loss: 0.04740829020738602\n",
            "Target Updated\n",
            "Episode: 2000/100000 DONE!\n",
            "Payoffs: [0.02, -0.78, -0.66, -0.58]\n",
            "Estimator Saved\n",
            "Episode: 2100/100000 DONE!\n",
            "Step: 180000 Loss: 0.03224426507949829\n",
            "Target Updated\n",
            "Payoffs: [-0.26, -0.64, -0.56, -0.56]\n",
            "Step: 180000 Loss: 0.027541982010006905\n",
            "Target Updated\n",
            "Episode: 2100/100000 DONE!\n",
            "Payoffs: [0.12, -0.66, -0.72, -0.74]\n",
            "Episode: 2200/100000 DONE!\n",
            "Step: 190000 Loss: 0.027195429429411888\n",
            "Target Updated\n",
            "Payoffs: [-0.06, -0.56, -0.68, -0.72]\n",
            "Step: 190000 Loss: 0.03788049519062042\n",
            "Target Updated\n",
            "Episode: 2200/100000 DONE!\n",
            "Payoffs: [-0.02, -0.66, -0.72, -0.6]\n",
            "Episode: 2300/100000 DONE!\n",
            "Step: 200000 Loss: 0.027199482545256615\n",
            "Target Updated\n",
            "Payoffs: [-0.18, -0.66, -0.5, -0.66]\n",
            "Step: 200000 Loss: 0.033506862819194794\n",
            "Target Updated\n",
            "Episode: 2300/100000 DONE!\n",
            "Payoffs: [-0.22, -0.56, -0.66, -0.56]\n",
            "Step: 210000 Loss: 0.024550147354602814\n",
            "Target Updated\n",
            "Episode: 2400/100000 DONE!\n",
            "Step: 210000 Loss: 0.026705360040068626\n",
            "Target Updated\n",
            "Payoffs: [0.0, -0.64, -0.68, -0.7]\n",
            "Episode: 2400/100000 DONE!\n",
            "Payoffs: [-0.08, -0.62, -0.58, -0.72]\n",
            "Step: 220000 Loss: 0.033390920609235764\n",
            "Target Updated\n",
            "Step: 220000 Loss: 0.03306535631418228\n",
            "Target Updated\n",
            "Episode: 2500/100000 DONE!\n",
            "Payoffs: [0.06, -0.62, -0.72, -0.74]\n",
            "Episode: 2500/100000 DONE!\n",
            "Payoffs: [-0.12, -0.56, -0.54, -0.78]\n",
            "Step: 230000 Loss: 0.030757905915379524\n",
            "Target Updated\n",
            "Step: 230000 Loss: 0.043471843004226685\n",
            "Target Updated\n",
            "Episode: 2600/100000 DONE!\n",
            "Episode: 2600/100000 DONE!\n",
            "Payoffs: [0.06, -0.72, -0.64, -0.7]\n",
            "Payoffs: [-0.04, -0.64, -0.5, -0.82]\n",
            "Step: 240000 Loss: 0.03693404048681259\n",
            "Target Updated\n",
            "Step: 240000 Loss: 0.03009859472513199\n",
            "Target Updated\n",
            "Episode: 2700/100000 DONE!\n",
            "Episode: 2700/100000 DONE!\n",
            "Payoffs: [-0.02, -0.6, -0.7, -0.68]\n",
            "Payoffs: [-0.06, -0.62, -0.68, -0.64]\n",
            "Step: 250000 Loss: 0.03191835433244705\n",
            "Target Updated\n",
            "Step: 250000 Loss: 0.030960846692323685\n",
            "Target Updated\n",
            "Episode: 2800/100000 DONE!\n",
            "Episode: 2800/100000 DONE!\n",
            "Payoffs: [-0.1, -0.66, -0.6, -0.68]\n",
            "Payoffs: [-0.06, -0.68, -0.64, -0.62]\n",
            "Step: 260000 Loss: 0.02448657713830471\n",
            "Target Updated\n",
            "Step: 260000 Loss: 0.026847682893276215\n",
            "Target Updated\n",
            "Episode: 2900/100000 DONE!\n",
            "Episode: 2900/100000 DONE!\n",
            "Payoffs: [-0.08, -0.56, -0.7, -0.66]\n",
            "Payoffs: [-0.04, -0.78, -0.5, -0.7]\n",
            "Step: 270000 Loss: 0.031440068036317825\n",
            "Target Updated\n",
            "Step: 270000 Loss: 0.026909220963716507\n",
            "Target Updated\n",
            "Episode: 3000/100000 DONE!\n",
            "Episode: 3000/100000 DONE!\n",
            "Payoffs: [-0.12, -0.58, -0.72, -0.58]\n",
            "Estimator Saved\n",
            "Payoffs: [0.16, -0.72, -0.72, -0.72]\n",
            "Estimator Saved\n",
            "Step: 280000 Loss: 0.03903857618570328\n",
            "Target Updated\n",
            "Step: 280000 Loss: 0.029127087444067\n",
            "Target Updated\n",
            "Episode: 3100/100000 DONE!\n",
            "Payoffs: [0.18, -0.74, -0.72, -0.74]\n",
            "Episode: 3100/100000 DONE!\n",
            "Payoffs: [-0.12, -0.6, -0.74, -0.54]\n",
            "Step: 290000 Loss: 0.018597975373268127\n",
            "Target Updated\n",
            "Step: 290000 Loss: 0.03159548342227936\n",
            "Target Updated\n",
            "Episode: 3200/100000 DONE!\n",
            "Payoffs: [0.0, -0.66, -0.76, -0.62]\n",
            "Step: 300000 Loss: 0.031556032598018646\n",
            "Target Updated\n",
            "Episode: 3200/100000 DONE!\n",
            "Step: 300000 Loss: 0.03237044811248779\n",
            "Target Updated\n",
            "Payoffs: [0.1, -0.66, -0.68, -0.76]\n",
            "Episode: 3300/100000 DONE!\n",
            "Payoffs: [0.08, -0.64, -0.84, -0.6]\n",
            "Step: 310000 Loss: 0.029245883226394653\n",
            "Target Updated\n",
            "Step: 310000 Loss: 0.041718751192092896\n",
            "Target Updated\n",
            "Episode: 3300/100000 DONE!\n",
            "Payoffs: [-0.08, -0.7, -0.48, -0.74]\n",
            "Episode: 3400/100000 DONE!\n",
            "Step: 320000 Loss: 0.032381571829319\n",
            "Target Updated\n",
            "Payoffs: [0.14, -0.74, -0.74, -0.68]\n",
            "Step: 320000 Loss: 0.02687927335500717\n",
            "Target Updated\n",
            "Episode: 3400/100000 DONE!\n",
            "Payoffs: [-0.18, -0.64, -0.54, -0.64]\n",
            "Step: 330000 Loss: 0.039441414177417755\n",
            "Target Updated\n",
            "Episode: 3500/100000 DONE!\n",
            "Step: 330000 Loss: 0.038763340562582016\n",
            "Target Updated\n",
            "Payoffs: [0.04, -0.76, -0.64, -0.64]\n",
            "Episode: 3500/100000 DONE!\n",
            "Payoffs: [0.0, -0.56, -0.78, -0.66]\n",
            "Step: 340000 Loss: 0.024664929136633873\n",
            "Target Updated\n",
            "Episode: 3600/100000 DONE!\n",
            "Step: 340000 Loss: 0.0346042737364769\n",
            "Target Updated\n",
            "Payoffs: [0.18, -0.64, -0.76, -0.78]\n",
            "Episode: 3600/100000 DONE!\n",
            "Payoffs: [0.18, -0.78, -0.76, -0.66]\n",
            "Step: 350000 Loss: 0.02869642898440361\n",
            "Target Updated\n",
            "Episode: 3700/100000 DONE!\n",
            "Step: 350000 Loss: 0.03401536867022514\n",
            "Target Updated\n",
            "Payoffs: [-0.08, -0.58, -0.72, -0.62]\n",
            "Episode: 3700/100000 DONE!\n",
            "Step: 360000 Loss: 0.03654131665825844\n",
            "Target Updated\n",
            "Episode: 3800/100000 DONE!\n",
            "Payoffs: [-0.14, -0.7, -0.68, -0.5]\n",
            "Step: 360000 Loss: 0.031623780727386475\n",
            "Target Updated\n",
            "Payoffs: [-0.02, -0.68, -0.76, -0.56]\n",
            "Step: 370000 Loss: 0.032240185886621475\n",
            "Target Updated\n",
            "Step: 370000 Loss: 0.029857145622372627\n",
            "Target Updated\n",
            "Episode: 3900/100000 DONE!\n",
            "Episode: 3800/100000 DONE!\n",
            "Payoffs: [0.02, -0.74, -0.56, -0.72]\n",
            "Payoffs: [0.02, -0.6, -0.72, -0.7]\n",
            "Episode: 4000/100000 DONE!\n",
            "Step: 380000 Loss: 0.03469725698232651\n",
            "Target Updated\n",
            "Payoffs: [-0.14, -0.48, -0.64, -0.74]\n",
            "Estimator Saved\n",
            "Step: 380000 Loss: 0.030994802713394165\n",
            "Target Updated\n",
            "Episode: 3900/100000 DONE!\n",
            "Payoffs: [-0.06, -0.58, -0.72, -0.66]\n",
            "Episode: 4100/100000 DONE!\n",
            "Step: 390000 Loss: 0.026266049593687057\n",
            "Target Updated\n",
            "Payoffs: [0.04, -0.62, -0.72, -0.72]\n",
            "Step: 390000 Loss: 0.0352950245141983\n",
            "Target Updated\n",
            "Episode: 4000/100000 DONE!\n",
            "Payoffs: [-0.18, -0.64, -0.62, -0.58]\n",
            "Estimator Saved\n",
            "Episode: 4200/100000 DONE!\n",
            "Payoffs: [-0.12, -0.54, -0.62, -0.74]\n",
            "Step: 400000 Loss: 0.03988948464393616\n",
            "Target Updated\n",
            "Step: 400000 Loss: 0.03637218475341797\n",
            "Target Updated\n",
            "Episode: 4100/100000 DONE!\n",
            "Episode: 4300/100000 DONE!\n",
            "Payoffs: [-0.12, -0.66, -0.72, -0.5]\n",
            "Payoffs: [-0.2, -0.5, -0.66, -0.64]\n",
            "Step: 410000 Loss: 0.026472946628928185\n",
            "Target Updated\n",
            "Step: 410000 Loss: 0.03030174970626831\n",
            "Target Updated\n",
            "Episode: 4400/100000 DONE!\n",
            "Payoffs: [0.04, -0.76, -0.58, -0.7]\n",
            "Episode: 4200/100000 DONE!\n",
            "Payoffs: [-0.22, -0.54, -0.74, -0.5]\n",
            "Step: 420000 Loss: 0.03227871283888817\n",
            "Target Updated\n",
            "Step: 420000 Loss: 0.031009607017040253\n",
            "Target Updated\n",
            "Episode: 4500/100000 DONE!\n",
            "Payoffs: [-0.02, -0.62, -0.64, -0.76]\n",
            "Step: 430000 Loss: 0.022563248872756958\n",
            "Target Updated\n",
            "Episode: 4300/100000 DONE!\n",
            "Step: 430000 Loss: 0.03059449978172779\n",
            "Target Updated\n",
            "Payoffs: [-0.06, -0.68, -0.7, -0.58]\n",
            "Episode: 4600/100000 DONE!\n",
            "Payoffs: [0.14, -0.6, -0.78, -0.76]\n",
            "Step: 440000 Loss: 0.01900973543524742\n",
            "Target Updated\n",
            "Episode: 4400/100000 DONE!\n",
            "Step: 440000 Loss: 0.030214212834835052\n",
            "Target Updated\n",
            "Payoffs: [-0.1, -0.66, -0.6, -0.64]\n",
            "Episode: 4700/100000 DONE!\n",
            "Payoffs: [-0.1, -0.66, -0.52, -0.72]\n",
            "Step: 450000 Loss: 0.024725638329982758\n",
            "Target Updated\n",
            "Step: 450000 Loss: 0.028033103793859482\n",
            "Target Updated\n",
            "Episode: 4500/100000 DONE!\n",
            "Payoffs: [0.08, -0.58, -0.78, -0.72]\n",
            "Episode: 4800/100000 DONE!\n",
            "Payoffs: [-0.18, -0.68, -0.56, -0.6]\n",
            "Step: 460000 Loss: 0.04054276645183563\n",
            "Target Updated\n",
            "Step: 460000 Loss: 0.030702192336320877\n",
            "Target Updated\n",
            "Episode: 4600/100000 DONE!\n",
            "Episode: 4900/100000 DONE!\n",
            "Payoffs: [-0.06, -0.64, -0.74, -0.56]\n",
            "Payoffs: [-0.14, -0.64, -0.54, -0.68]\n",
            "Step: 470000 Loss: 0.02962775155901909\n",
            "Target Updated\n",
            "Step: 470000 Loss: 0.026685144752264023\n",
            "Target Updated\n",
            "Episode: 5000/100000 DONE!\n",
            "Payoffs: [-0.16, -0.74, -0.56, -0.54]\n",
            "Estimator Saved\n",
            "Episode: 4700/100000 DONE!\n",
            "Payoffs: [-0.06, -0.72, -0.52, -0.72]\n",
            "Step: 480000 Loss: 0.020209960639476776\n",
            "Target Updated\n",
            "Step: 480000 Loss: 0.035180967301130295\n",
            "Target Updated\n",
            "Episode: 5100/100000 DONE!\n",
            "Payoffs: [-0.1, -0.7, -0.6, -0.6]\n",
            "Step: 490000 Loss: 0.02279822528362274\n",
            "Target Updated\n",
            "Episode: 4800/100000 DONE!\n",
            "Step: 490000 Loss: 0.026049431413412094\n",
            "Target Updated\n",
            "Episode: 5200/100000 DONE!\n",
            "Payoffs: [-0.1, -0.68, -0.66, -0.58]\n",
            "Step: 500000 Loss: 0.024567775428295135\n",
            "Target Updated\n",
            "Episode: 4900/100000 DONE!\n",
            "Step: 500000 Loss: 0.03269840404391289\n",
            "Target Updated\n",
            "Payoffs: [-0.06, -0.7, -0.62, -0.62]\n",
            "Episode: 5300/100000 DONE!\n",
            "Payoffs: [-0.14, -0.64, -0.66, -0.6]\n",
            "Step: 510000 Loss: 0.02925853803753853\n",
            "Target Updated\n",
            "Episode: 5000/100000 DONE!\n",
            "Step: 510000 Loss: 0.03840302675962448\n",
            "Target Updated\n",
            "Payoffs: [-0.06, -0.72, -0.54, -0.68]\n",
            "Estimator Saved\n",
            "Episode: 5400/100000 DONE!\n",
            "Payoffs: [-0.12, -0.64, -0.52, -0.74]\n",
            "Step: 520000 Loss: 0.03302877023816109\n",
            "Target Updated\n",
            "Step: 520000 Loss: 0.02960173785686493\n",
            "Target Updated\n",
            "Episode: 5100/100000 DONE!\n",
            "Episode: 5500/100000 DONE!\n",
            "Payoffs: [-0.1, -0.68, -0.52, -0.7]\n",
            "Payoffs: [0.08, -0.68, -0.7, -0.7]\n",
            "Step: 530000 Loss: 0.042192429304122925\n",
            "Target Updated\n",
            "Step: 530000 Loss: 0.033156298100948334\n",
            "Target Updated\n",
            "Episode: 5600/100000 DONE!\n",
            "Episode: 5200/100000 DONE!\n",
            "Payoffs: [-0.12, -0.68, -0.6, -0.6]\n",
            "Payoffs: [-0.08, -0.58, -0.68, -0.66]\n",
            "Step: 540000 Loss: 0.03525973856449127\n",
            "Target Updated\n",
            "Step: 540000 Loss: 0.035613950341939926\n",
            "Target Updated\n",
            "Episode: 5700/100000 DONE!\n",
            "Step: 550000 Loss: 0.025662047788500786\n",
            "Target Updated\n",
            "Payoffs: [-0.06, -0.58, -0.58, -0.8]\n",
            "Episode: 5300/100000 DONE!\n",
            "Payoffs: [0.04, -0.68, -0.68, -0.68]\n",
            "Step: 550000 Loss: 0.032926324754953384\n",
            "Target Updated\n",
            "Episode: 5800/100000 DONE!\n",
            "Step: 560000 Loss: 0.03565250337123871\n",
            "Target Updated\n",
            "Payoffs: [0.08, -0.74, -0.76, -0.58]\n",
            "Episode: 5400/100000 DONE!\n",
            "Payoffs: [0.1, -0.66, -0.72, -0.76]\n",
            "Step: 560000 Loss: 0.03752860426902771\n",
            "Target Updated\n",
            "Episode: 5900/100000 DONE!\n",
            "Step: 570000 Loss: 0.03262919932603836\n",
            "Target Updated\n",
            "Payoffs: [-0.14, -0.58, -0.76, -0.54]\n",
            "Episode: 5500/100000 DONE!\n",
            "Payoffs: [-0.02, -0.8, -0.58, -0.6]\n",
            "Step: 570000 Loss: 0.035303425043821335\n",
            "Target Updated\n",
            "Episode: 6000/100000 DONE!\n",
            "Step: 580000 Loss: 0.030836809426546097\n",
            "Target Updated\n",
            "Payoffs: [-0.06, -0.62, -0.64, -0.7]\n",
            "Estimator Saved\n",
            "Episode: 5600/100000 DONE!\n",
            "Payoffs: [0.14, -0.78, -0.76, -0.62]\n",
            "Step: 580000 Loss: 0.03107443079352379\n",
            "Target Updated\n",
            "Step: 590000 Loss: 0.04839552193880081\n",
            "Target Updated\n",
            "Episode: 6100/100000 DONE!\n",
            "Payoffs: [-0.1, -0.62, -0.72, -0.6]\n",
            "Episode: 5700/100000 DONE!\n",
            "Step: 590000 Loss: 0.03064911812543869\n",
            "Target Updated\n",
            "Payoffs: [-0.1, -0.7, -0.6, -0.68]\n",
            "Step: 600000 Loss: 0.024358853697776794\n",
            "Target Updated\n",
            "Episode: 6200/100000 DONE!\n",
            "Payoffs: [0.1, -0.74, -0.7, -0.66]\n",
            "Step: 600000 Loss: 0.03363071009516716\n",
            "Target Updated\n",
            "Episode: 5800/100000 DONE!\n",
            "Payoffs: [-0.12, -0.66, -0.6, -0.62]\n",
            "Step: 610000 Loss: 0.02850942313671112\n",
            "Target Updated\n",
            "Episode: 6300/100000 DONE!\n",
            "Payoffs: [-0.02, -0.58, -0.64, -0.76]\n",
            "Step: 610000 Loss: 0.023904791101813316\n",
            "Target Updated\n",
            "Step: 620000 Loss: 0.037930116057395935\n",
            "Target Updated\n",
            "Episode: 5900/100000 DONE!\n",
            "Payoffs: [-0.08, -0.54, -0.68, -0.72]\n",
            "Episode: 6400/100000 DONE!\n",
            "Payoffs: [0.0, -0.64, -0.66, -0.74]\n",
            "Step: 620000 Loss: 0.02423706464469433\n",
            "Target Updated\n",
            "Step: 630000 Loss: 0.030770353972911835\n",
            "Target Updated\n",
            "Episode: 6000/100000 DONE!\n",
            "Payoffs: [0.1, -0.76, -0.8, -0.54]\n",
            "Estimator Saved\n",
            "Episode: 6500/100000 DONE!\n",
            "Payoffs: [-0.02, -0.66, -0.66, -0.68]\n",
            "Step: 630000 Loss: 0.02185237780213356\n",
            "Target Updated\n",
            "Step: 640000 Loss: 0.03363056480884552\n",
            "Target Updated\n",
            "Episode: 6100/100000 DONE!\n",
            "Payoffs: [-0.14, -0.7, -0.68, -0.48]\n",
            "Step: 640000 Loss: 0.029992977157235146\n",
            "Target Updated\n",
            "Episode: 6600/100000 DONE!\n",
            "Payoffs: [-0.08, -0.8, -0.56, -0.56]\n",
            "Step: 650000 Loss: 0.023860003799200058\n",
            "Target Updated\n",
            "Episode: 6200/100000 DONE!\n",
            "Payoffs: [-0.04, -0.74, -0.68, -0.54]\n",
            "Step: 650000 Loss: 0.035872239619493484\n",
            "Target Updated\n",
            "Episode: 6700/100000 DONE!\n",
            "Step: 660000 Loss: 0.04088185355067253\n",
            "Target Updated\n",
            "Payoffs: [0.0, -0.8, -0.64, -0.6]\n",
            "Episode: 6300/100000 DONE!\n",
            "Payoffs: [-0.22, -0.7, -0.6, -0.48]\n",
            "Step: 660000 Loss: 0.033624567091464996\n",
            "Target Updated\n",
            "Step: 670000 Loss: 0.028861187398433685\n",
            "Target Updated\n",
            "Episode: 6800/100000 DONE!\n",
            "Payoffs: [0.0, -0.76, -0.7, -0.54]\n",
            "Episode: 6400/100000 DONE!\n",
            "Payoffs: [0.0, -0.7, -0.54, -0.76]\n",
            "Step: 670000 Loss: 0.02724708989262581\n",
            "Target Updated\n",
            "Step: 680000 Loss: 0.038608718663454056\n",
            "Target Updated\n",
            "Episode: 6900/100000 DONE!\n",
            "Payoffs: [-0.04, -0.72, -0.62, -0.64]\n",
            "Episode: 6500/100000 DONE!\n",
            "Step: 680000 Loss: 0.02578161656856537\n",
            "Target Updated\n",
            "Payoffs: [-0.02, -0.66, -0.6, -0.72]\n",
            "Step: 690000 Loss: 0.018425587564706802\n",
            "Target Updated\n",
            "Episode: 7000/100000 DONE!\n",
            "Payoffs: [0.1, -0.66, -0.76, -0.68]\n",
            "Estimator Saved\n",
            "Step: 690000 Loss: 0.026772325858473778\n",
            "Target Updated\n",
            "Step: 700000 Loss: 0.031303830444812775\n",
            "Target Updated\n",
            "Episode: 6600/100000 DONE!\n",
            "Payoffs: [-0.14, -0.62, -0.7, -0.54]\n",
            "Step: 700000 Loss: 0.024797435849905014\n",
            "Target Updated\n",
            "Episode: 7100/100000 DONE!\n",
            "Payoffs: [0.02, -0.62, -0.62, -0.8]\n",
            "Step: 710000 Loss: 0.032879769802093506\n",
            "Target Updated\n",
            "Episode: 6700/100000 DONE!\n",
            "Payoffs: [-0.18, -0.66, -0.7, -0.46]\n",
            "Step: 710000 Loss: 0.03745878487825394\n",
            "Target Updated\n",
            "Episode: 7200/100000 DONE!\n",
            "Step: 720000 Loss: 0.02912835031747818\n",
            "Target Updated\n",
            "Payoffs: [0.04, -0.64, -0.74, -0.68]\n",
            "Episode: 6800/100000 DONE!\n",
            "Payoffs: [-0.26, -0.82, -0.44, -0.48]\n",
            "Step: 720000 Loss: 0.02919510193169117\n",
            "Target Updated\n",
            "Step: 730000 Loss: 0.029858751222491264\n",
            "Target Updated\n",
            "Episode: 7300/100000 DONE!\n",
            "Episode: 6900/100000 DONE!\n",
            "Payoffs: [0.1, -0.66, -0.76, -0.68]\n",
            "Payoffs: [-0.24, -0.64, -0.54, -0.58]\n",
            "Step: 730000 Loss: 0.02941652201116085\n",
            "Target Updated\n",
            "Step: 740000 Loss: 0.028611190617084503\n",
            "Target Updated\n",
            "Episode: 7400/100000 DONE!\n",
            "Episode: 7000/100000 DONE!\n",
            "Payoffs: [-0.16, -0.46, -0.76, -0.68]\n",
            "Payoffs: [-0.08, -0.54, -0.76, -0.64]\n",
            "Estimator Saved\n",
            "Step: 740000 Loss: 0.02813790924847126\n",
            "Target Updated\n",
            "Step: 750000 Loss: 0.03060787357389927\n",
            "Target Updated\n",
            "Episode: 7500/100000 DONE!\n",
            "Episode: 7100/100000 DONE!\n",
            "Payoffs: [0.08, -0.72, -0.7, -0.7]\n",
            "Payoffs: [0.18, -0.78, -0.66, -0.74]\n",
            "Step: 750000 Loss: 0.026977790519595146\n",
            "Target Updated\n",
            "Step: 760000 Loss: 0.02651243284344673\n",
            "Target Updated\n",
            "Step: 760000 Loss: 0.04168480262160301\n",
            "Target Updated\n",
            "Episode: 7600/100000 DONE!\n",
            "Step: 770000 Loss: 0.031203780323266983\n",
            "Target Updated\n",
            "Episode: 7200/100000 DONE!\n",
            "Payoffs: [-0.1, -0.68, -0.68, -0.56]\n",
            "Payoffs: [0.18, -0.7, -0.64, -0.86]\n",
            "Step: 770000 Loss: 0.028902189806103706\n",
            "Target Updated\n",
            "Step: 780000 Loss: 0.03518527001142502\n",
            "Target Updated\n",
            "Episode: 7700/100000 DONE!\n",
            "Episode: 7300/100000 DONE!\n",
            "Payoffs: [0.02, -0.72, -0.64, -0.66]\n",
            "Payoffs: [0.12, -0.78, -0.58, -0.76]\n",
            "Step: 780000 Loss: 0.027499083429574966\n",
            "Target Updated\n",
            "Step: 790000 Loss: 0.0327172577381134\n",
            "Target Updated\n",
            "Episode: 7800/100000 DONE!\n",
            "Payoffs: [-0.06, -0.6, -0.68, -0.68]\n",
            "Episode: 7400/100000 DONE!\n",
            "Payoffs: [0.0, -0.7, -0.72, -0.58]\n",
            "Step: 790000 Loss: 0.028316842392086983\n",
            "Target Updated\n",
            "Step: 800000 Loss: 0.026975398883223534\n",
            "Target Updated\n",
            "Episode: 7900/100000 DONE!\n",
            "Payoffs: [-0.18, -0.62, -0.58, -0.62]\n",
            "Episode: 7500/100000 DONE!\n",
            "Payoffs: [-0.12, -0.72, -0.64, -0.54]\n",
            "Step: 800000 Loss: 0.031304847449064255\n",
            "Target Updated\n",
            "Step: 810000 Loss: 0.028452124446630478\n",
            "Target Updated\n",
            "Episode: 8000/100000 DONE!\n",
            "Payoffs: [-0.08, -0.56, -0.76, -0.62]\n",
            "Estimator Saved\n",
            "Step: 820000 Loss: 0.035639725625514984\n",
            "Target Updated\n",
            "Episode: 7600/100000 DONE!\n",
            "Step: 810000 Loss: 0.02080128714442253\n",
            "Target Updated\n",
            "Payoffs: [0.02, -0.76, -0.6, -0.66]\n",
            "Episode: 8100/100000 DONE!\n",
            "Payoffs: [-0.18, -0.6, -0.62, -0.6]\n",
            "Step: 830000 Loss: 0.024966761469841003\n",
            "Target Updated\n",
            "Step: 820000 Loss: 0.019056860357522964\n",
            "Target Updated\n",
            "Episode: 7700/100000 DONE!\n",
            "Payoffs: [0.1, -0.62, -0.76, -0.72]\n",
            "Episode: 8200/100000 DONE!\n",
            "Payoffs: [0.02, -0.62, -0.66, -0.74]\n",
            "Step: 840000 Loss: 0.04276859015226364\n",
            "Target Updated\n",
            "Step: 830000 Loss: 0.03409595414996147\n",
            "Target Updated\n",
            "Episode: 7800/100000 DONE!\n",
            "Payoffs: [-0.02, -0.74, -0.7, -0.54]\n",
            "Step: 840000 Loss: 0.030802853405475616\n",
            "Target Updated\n",
            "Step: 850000 Loss: 0.037368759512901306\n",
            "Target Updated\n",
            "Episode: 8300/100000 DONE!\n",
            "Payoffs: [0.04, -0.74, -0.7, -0.6]\n",
            "Episode: 7900/100000 DONE!\n",
            "Payoffs: [-0.22, -0.62, -0.62, -0.56]\n",
            "Step: 850000 Loss: 0.030394073575735092\n",
            "Target Updated\n",
            "Step: 860000 Loss: 0.020169518887996674\n",
            "Target Updated\n",
            "Episode: 8400/100000 DONE!\n",
            "Payoffs: [0.04, -0.72, -0.72, -0.6]\n",
            "Episode: 8000/100000 DONE!\n",
            "Step: 860000 Loss: 0.02600724622607231\n",
            "Target Updated\n",
            "Payoffs: [-0.04, -0.7, -0.66, -0.62]\n",
            "Estimator Saved\n",
            "Step: 870000 Loss: 0.040758609771728516\n",
            "Target Updated\n",
            "Episode: 8500/100000 DONE!\n",
            "Payoffs: [-0.16, -0.64, -0.68, -0.54]\n",
            "Step: 880000 Loss: 0.03295467793941498\n",
            "Target Updated\n",
            "Step: 870000 Loss: 0.03209654986858368\n",
            "Target Updated\n",
            "Episode: 8100/100000 DONE!\n",
            "Payoffs: [-0.14, -0.6, -0.66, -0.62]\n",
            "Episode: 8600/100000 DONE!\n",
            "Payoffs: [-0.02, -0.66, -0.68, -0.66]\n",
            "Step: 890000 Loss: 0.03129005432128906\n",
            "Target Updated\n",
            "Step: 880000 Loss: 0.027222195640206337\n",
            "Target Updated\n",
            "Episode: 8200/100000 DONE!\n",
            "Payoffs: [-0.28, -0.6, -0.58, -0.54]\n",
            "Episode: 8700/100000 DONE!\n",
            "Step: 900000 Loss: 0.02059520035982132\n",
            "Target Updated\n",
            "Payoffs: [0.12, -0.68, -0.6, -0.84]\n",
            "Step: 890000 Loss: 0.024841271340847015\n",
            "Target Updated\n",
            "Episode: 8300/100000 DONE!\n",
            "Payoffs: [0.1, -0.82, -0.64, -0.64]\n",
            "Step: 900000 Loss: 0.020335281267762184\n",
            "Target Updated\n",
            "Step: 910000 Loss: 0.020853973925113678\n",
            "Target Updated\n",
            "Episode: 8800/100000 DONE!\n",
            "Payoffs: [-0.06, -0.66, -0.62, -0.68]\n",
            "Episode: 8400/100000 DONE!\n",
            "Payoffs: [-0.04, -0.6, -0.66, -0.7]\n",
            "Step: 910000 Loss: 0.034415461122989655\n",
            "Target Updated\n",
            "Step: 920000 Loss: 0.029395800083875656\n",
            "Target Updated\n",
            "Episode: 8900/100000 DONE!\n",
            "Payoffs: [-0.02, -0.6, -0.66, -0.74]\n",
            "Step: 930000 Loss: 0.035387858748435974\n",
            "Target Updated\n",
            "Episode: 8500/100000 DONE!\n",
            "Step: 920000 Loss: 0.02830522134900093\n",
            "Target Updated\n",
            "Payoffs: [-0.16, -0.56, -0.52, -0.78]\n",
            "Episode: 9000/100000 DONE!\n",
            "Payoffs: [0.04, -0.68, -0.68, -0.68]\n",
            "Estimator Saved\n",
            "Step: 940000 Loss: 0.03182876855134964\n",
            "Target Updated\n",
            "Step: 930000 Loss: 0.02010870724916458\n",
            "Target Updated\n",
            "Episode: 8600/100000 DONE!\n",
            "Payoffs: [-0.12, -0.82, -0.66, -0.42]\n",
            "Episode: 9100/100000 DONE!\n",
            "Step: 950000 Loss: 0.019756322726607323\n",
            "Target Updated\n",
            "Payoffs: [-0.2, -0.58, -0.7, -0.54]\n",
            "Step: 940000 Loss: 0.030680671334266663\n",
            "Target Updated\n",
            "Episode: 8700/100000 DONE!\n",
            "Payoffs: [-0.02, -0.76, -0.5, -0.74]\n",
            "Step: 950000 Loss: 0.02160419523715973\n",
            "Target Updated\n",
            "Episode: 9200/100000 DONE!\n",
            "Step: 960000 Loss: 0.02119877189397812\n",
            "Target Updated\n",
            "Payoffs: [-0.12, -0.64, -0.76, -0.52]\n",
            "Step: 970000 Loss: 0.036395322531461716\n",
            "Target Updated\n",
            "Step: 960000 Loss: 0.026315296068787575\n",
            "Target Updated\n",
            "Episode: 8800/100000 DONE!\n",
            "Episode: 9300/100000 DONE!\n",
            "Payoffs: [0.0, -0.72, -0.56, -0.76]\n",
            "Payoffs: [-0.04, -0.62, -0.72, -0.64]\n",
            "Step: 980000 Loss: 0.027246618643403053\n",
            "Target Updated\n",
            "Step: 970000 Loss: 0.03085090033710003\n",
            "Target Updated\n",
            "Episode: 9400/100000 DONE!\n",
            "Episode: 8900/100000 DONE!\n",
            "Payoffs: [0.14, -0.78, -0.68, -0.68]\n",
            "Payoffs: [-0.02, -0.66, -0.74, -0.58]\n",
            "Step: 990000 Loss: 0.03003733605146408\n",
            "Target Updated\n",
            "Step: 980000 Loss: 0.028997797518968582\n",
            "Target Updated\n",
            "Episode: 9500/100000 DONE!\n",
            "Payoffs: [-0.1, -0.7, -0.66, -0.54]\n",
            "Episode: 9000/100000 DONE!\n",
            "Payoffs: [-0.02, -0.6, -0.68, -0.7]\n",
            "Estimator Saved\n",
            "Step: 1000000 Loss: 0.017922118306159973\n",
            "Target Updated\n",
            "Step: 990000 Loss: 0.03314495459198952\n",
            "Target Updated\n",
            "Episode: 9600/100000 DONE!\n",
            "Payoffs: [-0.06, -0.64, -0.66, -0.66]\n",
            "Episode: 9100/100000 DONE!\n",
            "Payoffs: [-0.02, -0.72, -0.66, -0.62]\n",
            "Step: 1010000 Loss: 0.03312505781650543\n",
            "Target Updated\n",
            "Step: 1000000 Loss: 0.026221856474876404\n",
            "Target Updated\n",
            "Episode: 9700/100000 DONE!\n",
            "Payoffs: [0.02, -0.7, -0.68, -0.64]\n",
            "Step: 1020000 Loss: 0.035805247724056244\n",
            "Target Updated\n",
            "Episode: 9200/100000 DONE!\n",
            "Payoffs: [-0.12, -0.46, -0.7, -0.72]\n",
            "Step: 1010000 Loss: 0.02217281423509121\n",
            "Target Updated\n",
            "Episode: 9800/100000 DONE!\n",
            "Payoffs: [0.08, -0.68, -0.74, -0.68]\n",
            "Step: 1030000 Loss: 0.033043116331100464\n",
            "Target Updated\n",
            "Episode: 9300/100000 DONE!\n",
            "Step: 1020000 Loss: 0.028279263526201248\n",
            "Target Updated\n",
            "Payoffs: [-0.1, -0.74, -0.64, -0.52]\n",
            "Episode: 9900/100000 DONE!\n",
            "Payoffs: [-0.12, -0.68, -0.7, -0.5]\n",
            "Step: 1040000 Loss: 0.026219340041279793\n",
            "Target Updated\n",
            "Step: 1030000 Loss: 0.026793863624334335\n",
            "Target Updated\n",
            "Episode: 9400/100000 DONE!\n",
            "Payoffs: [0.02, -0.54, -0.68, -0.8]\n",
            "Episode: 10000/100000 DONE!\n",
            "Step: 1050000 Loss: 0.02808886393904686\n",
            "Target Updated\n",
            "Payoffs: [-0.08, -0.7, -0.54, -0.7]\n",
            "Estimator Saved\n",
            "Step: 1040000 Loss: 0.02754446305334568\n",
            "Target Updated\n",
            "Episode: 9500/100000 DONE!\n",
            "Payoffs: [0.06, -0.66, -0.74, -0.66]\n",
            "Step: 1060000 Loss: 0.028539620339870453\n",
            "Target Updated\n",
            "Episode: 10100/100000 DONE!\n",
            "Payoffs: [-0.06, -0.68, -0.66, -0.6]\n",
            "Step: 1050000 Loss: 0.026651352643966675\n",
            "Target Updated\n",
            "Step: 1070000 Loss: 0.0319158174097538\n",
            "Target Updated\n",
            "Step: 1060000 Loss: 0.020790841430425644\n",
            "Target Updated\n",
            "Episode: 9600/100000 DONE!\n",
            "Episode: 10200/100000 DONE!\n",
            "Payoffs: [-0.1, -0.56, -0.52, -0.82]\n",
            "Payoffs: [-0.02, -0.66, -0.72, -0.64]\n",
            "Step: 1080000 Loss: 0.030635427683591843\n",
            "Target Updated\n",
            "Step: 1070000 Loss: 0.037083618342876434\n",
            "Target Updated\n",
            "Episode: 10300/100000 DONE!\n",
            "Episode: 9700/100000 DONE!\n",
            "Payoffs: [0.12, -0.64, -0.78, -0.7]\n",
            "Payoffs: [-0.02, -0.76, -0.52, -0.72]\n",
            "Step: 1090000 Loss: 0.03514615446329117\n",
            "Target Updated\n",
            "Step: 1080000 Loss: 0.030127640813589096\n",
            "Target Updated\n",
            "Episode: 10400/100000 DONE!\n",
            "Step: 1100000 Loss: 0.02569221518933773\n",
            "Target Updated\n",
            "Payoffs: [-0.08, -0.56, -0.72, -0.66]\n",
            "Episode: 9800/100000 DONE!\n",
            "Payoffs: [0.1, -0.66, -0.78, -0.66]\n",
            "Step: 1090000 Loss: 0.0317358560860157\n",
            "Target Updated\n",
            "Step: 1110000 Loss: 0.0272110253572464\n",
            "Target Updated\n",
            "Episode: 10500/100000 DONE!\n",
            "Payoffs: [0.04, -0.72, -0.7, -0.66]\n",
            "Step: 1100000 Loss: 0.030920013785362244\n",
            "Target Updated\n",
            "Episode: 9900/100000 DONE!\n",
            "Payoffs: [-0.08, -0.72, -0.64, -0.56]\n",
            "Step: 1120000 Loss: 0.02458946593105793\n",
            "Target Updated\n",
            "Episode: 10600/100000 DONE!\n",
            "Payoffs: [0.16, -0.7, -0.64, -0.84]\n",
            "Step: 1110000 Loss: 0.02564030885696411\n",
            "Target Updated\n",
            "Episode: 10000/100000 DONE!\n",
            "Payoffs: [-0.06, -0.84, -0.64, -0.48]\n",
            "Estimator Saved\n",
            "Step: 1130000 Loss: 0.02448434755206108\n",
            "Target Updated\n",
            "Step: 1120000 Loss: 0.02127230353653431\n",
            "Target Updated\n",
            "Episode: 10700/100000 DONE!\n",
            "Payoffs: [0.1, -0.76, -0.74, -0.62]\n",
            "Step: 1140000 Loss: 0.02812052331864834\n",
            "Target Updated\n",
            "Episode: 10100/100000 DONE!\n",
            "Payoffs: [0.08, -0.74, -0.7, -0.66]\n",
            "Step: 1130000 Loss: 0.026529837399721146\n",
            "Target Updated\n",
            "Episode: 10800/100000 DONE!\n",
            "Payoffs: [0.04, -0.64, -0.74, -0.66]\n",
            "Step: 1150000 Loss: 0.02573855221271515\n",
            "Target Updated\n",
            "Step: 1140000 Loss: 0.03277966380119324\n",
            "Target Updated\n",
            "Episode: 10200/100000 DONE!\n",
            "Payoffs: [-0.08, -0.64, -0.68, -0.62]\n",
            "Episode: 10900/100000 DONE!\n",
            "Payoffs: [0.02, -0.76, -0.74, -0.54]\n",
            "Step: 1160000 Loss: 0.027445930987596512\n",
            "Target Updated\n",
            "Step: 1150000 Loss: 0.028295321390032768\n",
            "Target Updated\n",
            "Episode: 10300/100000 DONE!\n",
            "Payoffs: [-0.16, -0.56, -0.62, -0.66]\n",
            "Episode: 11000/100000 DONE!\n",
            "Step: 1170000 Loss: 0.022755756974220276\n",
            "Target Updated\n",
            "Payoffs: [0.36, -0.78, -0.76, -0.82]\n",
            "Estimator Saved\n",
            "Step: 1160000 Loss: 0.03159039840102196\n",
            "Target Updated\n",
            "Step: 1180000 Loss: 0.02273884415626526\n",
            "Target Updated\n",
            "Episode: 10400/100000 DONE!\n",
            "Episode: 11100/100000 DONE!\n",
            "Payoffs: [-0.02, -0.74, -0.6, -0.64]\n",
            "Payoffs: [0.04, -0.66, -0.84, -0.56]\n",
            "Step: 1170000 Loss: 0.015564636327326298\n",
            "Target Updated\n",
            "Step: 1190000 Loss: 0.021048448979854584\n",
            "Target Updated\n",
            "Step: 1180000 Loss: 0.029305819422006607\n",
            "Target Updated\n",
            "Episode: 10500/100000 DONE!\n",
            "Episode: 11200/100000 DONE!\n",
            "Payoffs: [-0.12, -0.72, -0.64, -0.56]\n",
            "Payoffs: [-0.04, -0.62, -0.62, -0.72]\n",
            "Step: 1200000 Loss: 0.028554845601320267\n",
            "Target Updated\n",
            "Step: 1190000 Loss: 0.024056922644376755\n",
            "Target Updated\n",
            "Step: 1210000 Loss: 0.021592210978269577\n",
            "Target Updated\n",
            "Episode: 11300/100000 DONE!\n",
            "Episode: 10600/100000 DONE!\n",
            "Payoffs: [-0.22, -0.58, -0.64, -0.56]\n",
            "Payoffs: [0.06, -0.6, -0.86, -0.62]\n",
            "Step: 1200000 Loss: 0.030584923923015594\n",
            "Target Updated\n",
            "Step: 1220000 Loss: 0.011751336045563221\n",
            "Target Updated\n",
            "Step: 1210000 Loss: 0.022565141320228577\n",
            "Target Updated\n",
            "Episode: 11400/100000 DONE!\n",
            "Episode: 10700/100000 DONE!\n",
            "Payoffs: [-0.2, -0.56, -0.68, -0.58]\n",
            "Payoffs: [-0.1, -0.68, -0.7, -0.58]\n",
            "Step: 1230000 Loss: 0.03033662773668766\n",
            "Target Updated\n",
            "Step: 1220000 Loss: 0.022019008174538612\n",
            "Target Updated\n",
            "Step: 1240000 Loss: 0.02210322394967079\n",
            "Target Updated\n",
            "Episode: 11500/100000 DONE!\n",
            "Episode: 10800/100000 DONE!\n",
            "Payoffs: [0.16, -0.62, -0.72, -0.82]\n",
            "Payoffs: [0.02, -0.66, -0.64, -0.72]\n",
            "Step: 1230000 Loss: 0.01943480223417282\n",
            "Target Updated\n",
            "Step: 1250000 Loss: 0.024811025708913803\n",
            "Target Updated\n",
            "Step: 1240000 Loss: 0.01970265805721283\n",
            "Target Updated\n",
            "Episode: 11600/100000 DONE!\n",
            "Episode: 10900/100000 DONE!\n",
            "Payoffs: [-0.04, -0.7, -0.62, -0.64]\n",
            "Payoffs: [-0.02, -0.74, -0.6, -0.64]\n",
            "Step: 1260000 Loss: 0.018908202648162842\n",
            "Target Updated\n",
            "Step: 1250000 Loss: 0.025432482361793518\n",
            "Target Updated\n",
            "Episode: 11000/100000 DONE!\n",
            "Episode: 11700/100000 DONE!\n",
            "Payoffs: [-0.08, -0.58, -0.68, -0.68]\n",
            "Estimator Saved\n",
            "Payoffs: [-0.08, -0.58, -0.74, -0.6]\n",
            "Step: 1270000 Loss: 0.03667977452278137\n",
            "Target Updated\n",
            "Step: 1260000 Loss: 0.030624747276306152\n",
            "Target Updated\n",
            "Step: 1280000 Loss: 0.0371965728700161\n",
            "Target Updated\n",
            "Episode: 11100/100000 DONE!\n",
            "Step: 1270000 Loss: 0.022856811061501503\n",
            "Target Updated\n",
            "Episode: 11800/100000 DONE!\n",
            "Payoffs: [-0.16, -0.68, -0.66, -0.52]\n",
            "Payoffs: [-0.14, -0.68, -0.66, -0.54]\n",
            "Step: 1290000 Loss: 0.015625717118382454\n",
            "Target Updated\n",
            "Step: 1280000 Loss: 0.03464271128177643\n",
            "Target Updated\n",
            "Episode: 11900/100000 DONE!\n",
            "Episode: 11200/100000 DONE!\n",
            "Payoffs: [0.06, -0.74, -0.72, -0.6]\n",
            "Payoffs: [-0.08, -0.8, -0.54, -0.58]\n",
            "Step: 1300000 Loss: 0.020539237186312675\n",
            "Target Updated\n",
            "Step: 1290000 Loss: 0.019212104380130768\n",
            "Target Updated\n",
            "Step: 1310000 Loss: 0.02007053792476654\n",
            "Target Updated\n",
            "Episode: 12000/100000 DONE!\n",
            "Episode: 11300/100000 DONE!\n",
            "Payoffs: [-0.14, -0.72, -0.56, -0.58]\n",
            "Estimator Saved\n",
            "Payoffs: [-0.14, -0.6, -0.62, -0.72]\n",
            "Step: 1300000 Loss: 0.015829281881451607\n",
            "Target Updated\n",
            "Step: 1320000 Loss: 0.020471088588237762\n",
            "Target Updated\n",
            "Step: 1310000 Loss: 0.020519044250249863\n",
            "Target Updated\n",
            "Episode: 12100/100000 DONE!\n",
            "Payoffs: [0.14, -0.72, -0.66, -0.76]\n",
            "Episode: 11400/100000 DONE!\n",
            "Payoffs: [-0.06, -0.72, -0.7, -0.54]\n",
            "Step: 1330000 Loss: 0.02090703696012497\n",
            "Target Updated\n",
            "Step: 1320000 Loss: 0.012859677895903587\n",
            "Target Updated\n",
            "Episode: 12200/100000 DONE!\n",
            "Step: 1340000 Loss: 0.024442512542009354\n",
            "Target Updated\n",
            "Payoffs: [0.1, -0.76, -0.78, -0.6]\n",
            "Episode: 11500/100000 DONE!\n",
            "Step: 1330000 Loss: 0.019713671877980232\n",
            "Target Updated\n",
            "Payoffs: [-0.1, -0.74, -0.6, -0.56]\n",
            "Step: 1350000 Loss: 0.02280977927148342\n",
            "Target Updated\n",
            "Step: 1340000 Loss: 0.019644396379590034\n",
            "Target Updated\n",
            "Episode: 12300/100000 DONE!\n",
            "Payoffs: [0.16, -0.78, -0.72, -0.66]\n",
            "Episode: 11600/100000 DONE!\n",
            "Payoffs: [0.1, -0.76, -0.64, -0.72]\n",
            "Step: 1360000 Loss: 0.03551904857158661\n",
            "Target Updated\n",
            "Step: 1350000 Loss: 0.022069958969950676\n",
            "Target Updated\n",
            "Episode: 12400/100000 DONE!\n",
            "Payoffs: [-0.06, -0.68, -0.66, -0.6]\n",
            "Step: 1370000 Loss: 0.01734287291765213\n",
            "Target Updated\n",
            "Episode: 11700/100000 DONE!\n",
            "Step: 1360000 Loss: 0.01756666973233223\n",
            "Target Updated\n",
            "Payoffs: [-0.1, -0.72, -0.7, -0.5]\n",
            "Step: 1380000 Loss: 0.01057886891067028\n",
            "Target Updated\n",
            "Episode: 12500/100000 DONE!\n",
            "Payoffs: [0.2, -0.76, -0.68, -0.76]\n",
            "Step: 1370000 Loss: 0.024633388966321945\n",
            "Target Updated\n",
            "Episode: 11800/100000 DONE!\n",
            "Payoffs: [0.0, -0.7, -0.74, -0.58]\n",
            "Step: 1390000 Loss: 0.023701481521129608\n",
            "Target Updated\n",
            "Step: 1380000 Loss: 0.030721843242645264\n",
            "Target Updated\n",
            "Episode: 12600/100000 DONE!\n",
            "Payoffs: [-0.08, -0.76, -0.6, -0.58]\n",
            "Step: 1400000 Loss: 0.018972907215356827\n",
            "Target Updated\n",
            "Episode: 11900/100000 DONE!\n",
            "Step: 1390000 Loss: 0.028968162834644318\n",
            "Target Updated\n",
            "Payoffs: [-0.04, -0.72, -0.62, -0.62]\n",
            "Episode: 12700/100000 DONE!\n",
            "Step: 1410000 Loss: 0.023437466472387314\n",
            "Target Updated\n",
            "Payoffs: [-0.08, -0.68, -0.6, -0.66]\n",
            "Step: 1400000 Loss: 0.02532116137444973\n",
            "Target Updated\n",
            "Episode: 12000/100000 DONE!\n",
            "Payoffs: [0.04, -0.74, -0.66, -0.66]\n",
            "Estimator Saved\n",
            "Step: 1420000 Loss: 0.037439510226249695\n",
            "Target Updated\n",
            "Step: 1410000 Loss: 0.01916045881807804\n",
            "Target Updated\n",
            "Episode: 12800/100000 DONE!\n",
            "Payoffs: [-0.04, -0.64, -0.58, -0.74]\n",
            "Step: 1430000 Loss: 0.021027253940701485\n",
            "Target Updated\n",
            "Episode: 12100/100000 DONE!\n",
            "Step: 1420000 Loss: 0.020678812637925148\n",
            "Target Updated\n",
            "Payoffs: [-0.12, -0.74, -0.6, -0.56]\n",
            "Step: 1440000 Loss: 0.017955753952264786\n",
            "Target Updated\n",
            "Episode: 12900/100000 DONE!\n",
            "Payoffs: [-0.16, -0.8, -0.56, -0.5]\n",
            "Step: 1430000 Loss: 0.023236392065882683\n",
            "Target Updated\n",
            "Episode: 12200/100000 DONE!\n",
            "Payoffs: [-0.02, -0.76, -0.64, -0.6]\n",
            "Step: 1450000 Loss: 0.03485636040568352\n",
            "Target Updated\n",
            "Step: 1440000 Loss: 0.027723342180252075\n",
            "Target Updated\n",
            "Episode: 13000/100000 DONE!\n",
            "Payoffs: [0.0, -0.66, -0.68, -0.68]\n",
            "Estimator Saved\n",
            "Episode: 12300/100000 DONE!\n",
            "Payoffs: [-0.02, -0.64, -0.66, -0.68]\n",
            "Step: 1460000 Loss: 0.01465518493205309\n",
            "Target Updated\n",
            "Step: 1450000 Loss: 0.0220320001244545\n",
            "Target Updated\n",
            "Episode: 13100/100000 DONE!\n",
            "Payoffs: [0.04, -0.72, -0.64, -0.7]\n",
            "Step: 1470000 Loss: 0.03810156136751175\n",
            "Target Updated\n",
            "Episode: 12400/100000 DONE!\n",
            "Step: 1460000 Loss: 0.022160114720463753\n",
            "Target Updated\n",
            "Payoffs: [-0.1, -0.68, -0.6, -0.64]\n",
            "Episode: 13200/100000 DONE!\n",
            "Step: 1480000 Loss: 0.029435453936457634\n",
            "Target Updated\n",
            "Payoffs: [-0.02, -0.62, -0.74, -0.64]\n",
            "Step: 1470000 Loss: 0.02186623029410839\n",
            "Target Updated\n",
            "Episode: 12500/100000 DONE!\n",
            "Payoffs: [0.0, -0.66, -0.76, -0.58]\n",
            "Step: 1490000 Loss: 0.02298864535987377\n",
            "Target Updated\n",
            "Step: 1480000 Loss: 0.021901337429881096\n",
            "Target Updated\n",
            "Episode: 13300/100000 DONE!\n",
            "Payoffs: [-0.26, -0.58, -0.64, -0.52]\n",
            "Step: 1500000 Loss: 0.020053833723068237\n",
            "Target Updated\n",
            "Episode: 12600/100000 DONE!\n",
            "Step: 1490000 Loss: 0.025891968980431557\n",
            "Target Updated\n",
            "Payoffs: [-0.14, -0.68, -0.64, -0.56]\n",
            "Episode: 13400/100000 DONE!\n",
            "Payoffs: [-0.02, -0.78, -0.58, -0.62]\n",
            "Step: 1510000 Loss: 0.022973764687776566\n",
            "Target Updated\n",
            "Step: 1500000 Loss: 0.018073227256536484\n",
            "Target Updated\n",
            "Episode: 12700/100000 DONE!\n",
            "Payoffs: [-0.18, -0.7, -0.7, -0.44]\n",
            "Episode: 13500/100000 DONE!\n",
            "Payoffs: [-0.08, -0.62, -0.68, -0.62]\n",
            "Step: 1520000 Loss: 0.026908423751592636\n",
            "Target Updated\n",
            "Step: 1510000 Loss: 0.029320476576685905\n",
            "Target Updated\n",
            "Episode: 12800/100000 DONE!\n",
            "Payoffs: [-0.04, -0.62, -0.76, -0.64]\n",
            "Step: 1530000 Loss: 0.025813842192292213\n",
            "Target Updated\n",
            "Episode: 13600/100000 DONE!\n",
            "Payoffs: [0.02, -0.74, -0.68, -0.6]\n",
            "Step: 1520000 Loss: 0.029923997819423676\n",
            "Target Updated\n",
            "Step: 1540000 Loss: 0.016907665878534317\n",
            "Target Updated\n",
            "Episode: 12900/100000 DONE!\n",
            "Step: 1530000 Loss: 0.017627233639359474\n",
            "Target Updated\n",
            "Payoffs: [0.02, -0.78, -0.64, -0.6]\n",
            "Episode: 13700/100000 DONE!\n",
            "Payoffs: [0.04, -0.78, -0.56, -0.72]\n",
            "Step: 1550000 Loss: 0.030663352459669113\n",
            "Target Updated\n",
            "Step: 1540000 Loss: 0.025725485756993294\n",
            "Target Updated\n",
            "Episode: 13000/100000 DONE!\n",
            "Episode: 13800/100000 DONE!\n",
            "Payoffs: [-0.14, -0.62, -0.54, -0.7]\n",
            "Estimator Saved\n",
            "Payoffs: [-0.08, -0.72, -0.52, -0.68]\n",
            "Step: 1560000 Loss: 0.019623704254627228\n",
            "Target Updated\n",
            "Step: 1550000 Loss: 0.03416094183921814\n",
            "Target Updated\n",
            "Episode: 13900/100000 DONE!\n",
            "Episode: 13100/100000 DONE!\n",
            "Payoffs: [0.12, -0.64, -0.72, -0.76]\n",
            "Payoffs: [-0.18, -0.56, -0.58, -0.68]\n",
            "Step: 1570000 Loss: 0.01609640009701252\n",
            "Target Updated\n",
            "Step: 1560000 Loss: 0.022386088967323303\n",
            "Target Updated\n",
            "Step: 1580000 Loss: 0.02934947982430458\n",
            "Target Updated\n",
            "Episode: 14000/100000 DONE!\n",
            "Payoffs: [-0.06, -0.62, -0.78, -0.56]\n",
            "Estimator Saved\n",
            "Episode: 13200/100000 DONE!\n",
            "Step: 1570000 Loss: 0.020534902811050415\n",
            "Target Updated\n",
            "Payoffs: [0.06, -0.78, -0.64, -0.68]\n",
            "Step: 1590000 Loss: 0.024776797741651535\n",
            "Target Updated\n",
            "Episode: 14100/100000 DONE!\n",
            "Payoffs: [-0.06, -0.68, -0.62, -0.64]\n",
            "Step: 1580000 Loss: 0.03000812605023384\n",
            "Target Updated\n",
            "Episode: 13300/100000 DONE!\n",
            "Payoffs: [-0.02, -0.76, -0.68, -0.6]\n",
            "Step: 1600000 Loss: 0.020989133045077324\n",
            "Target Updated\n",
            "Step: 1590000 Loss: 0.022080661728978157\n",
            "Target Updated\n",
            "Episode: 14200/100000 DONE!\n",
            "Payoffs: [0.12, -0.74, -0.64, -0.78]\n",
            "Step: 1610000 Loss: 0.015717562288045883\n",
            "Target Updated\n",
            "Episode: 13400/100000 DONE!\n",
            "Payoffs: [0.22, -0.84, -0.68, -0.7]\n",
            "Step: 1600000 Loss: 0.026536745950579643\n",
            "Target Updated\n",
            "Episode: 14300/100000 DONE!\n",
            "Payoffs: [-0.1, -0.72, -0.6, -0.6]\n",
            "Step: 1620000 Loss: 0.021335668861865997\n",
            "Target Updated\n",
            "Step: 1610000 Loss: 0.014016928151249886\n",
            "Target Updated\n",
            "Episode: 13500/100000 DONE!\n",
            "Payoffs: [0.06, -0.78, -0.7, -0.58]\n",
            "Episode: 14400/100000 DONE!\n",
            "Step: 1630000 Loss: 0.028308171778917313\n",
            "Target Updated\n",
            "Payoffs: [0.04, -0.74, -0.58, -0.72]\n",
            "Step: 1620000 Loss: 0.033616065979003906\n",
            "Target Updated\n",
            "Episode: 13600/100000 DONE!\n",
            "Payoffs: [-0.08, -0.7, -0.62, -0.62]\n",
            "Step: 1640000 Loss: 0.020981675013899803\n",
            "Target Updated\n",
            "Episode: 14500/100000 DONE!\n",
            "Payoffs: [-0.14, -0.6, -0.6, -0.68]\n",
            "Step: 1630000 Loss: 0.0222608745098114\n",
            "Target Updated\n",
            "Step: 1650000 Loss: 0.023955510929226875\n",
            "Target Updated\n",
            "Episode: 13700/100000 DONE!\n",
            "Step: 1640000 Loss: 0.027520474046468735\n",
            "Target Updated\n",
            "Payoffs: [-0.14, -0.6, -0.62, -0.66]\n",
            "Episode: 14600/100000 DONE!\n",
            "Payoffs: [0.08, -0.58, -0.74, -0.76]\n",
            "Step: 1660000 Loss: 0.02815483883023262\n",
            "Target Updated\n",
            "Episode: 13800/100000 DONE!\n",
            "Step: 1650000 Loss: 0.020551562309265137\n",
            "Target Updated\n",
            "Payoffs: [-0.02, -0.64, -0.62, -0.76]\n",
            "Episode: 14700/100000 DONE!\n",
            "Payoffs: [-0.04, -0.82, -0.6, -0.54]\n",
            "Step: 1670000 Loss: 0.04336042329668999\n",
            "Target Updated\n",
            "Step: 1660000 Loss: 0.019526220858097076\n",
            "Target Updated\n",
            "Episode: 13900/100000 DONE!\n",
            "Payoffs: [-0.06, -0.56, -0.72, -0.66]\n",
            "Episode: 14800/100000 DONE!\n",
            "Payoffs: [-0.02, -0.48, -0.74, -0.78]\n",
            "Step: 1680000 Loss: 0.027537580579519272\n",
            "Target Updated\n",
            "Step: 1670000 Loss: 0.026812490075826645\n",
            "Target Updated\n",
            "Episode: 14000/100000 DONE!\n",
            "Payoffs: [-0.08, -0.64, -0.6, -0.68]\n",
            "Estimator Saved\n",
            "Episode: 14900/100000 DONE!\n",
            "Payoffs: [-0.26, -0.74, -0.46, -0.54]\n",
            "Step: 1690000 Loss: 0.025429513305425644\n",
            "Target Updated\n",
            "Step: 1680000 Loss: 0.02276531420648098\n",
            "Target Updated\n",
            "Episode: 14100/100000 DONE!\n",
            "Episode: 15000/100000 DONE!\n",
            "Payoffs: [0.1, -0.62, -0.66, -0.82]\n",
            "Step: 1700000 Loss: 0.02917690947651863\n",
            "Target Updated\n",
            "Payoffs: [-0.1, -0.6, -0.62, -0.72]\n",
            "Estimator Saved\n",
            "Step: 1690000 Loss: 0.02931363694369793\n",
            "Target Updated\n",
            "Step: 1710000 Loss: 0.027199236676096916\n",
            "Target Updated\n",
            "Episode: 14200/100000 DONE!\n",
            "Episode: 15100/100000 DONE!\n",
            "Payoffs: [0.06, -0.74, -0.74, -0.6]\n",
            "Payoffs: [-0.1, -0.76, -0.46, -0.7]\n",
            "Step: 1700000 Loss: 0.016850009560585022\n",
            "Target Updated\n",
            "Step: 1720000 Loss: 0.021041613072156906\n",
            "Target Updated\n",
            "Episode: 15200/100000 DONE!\n",
            "Episode: 14300/100000 DONE!\n",
            "Payoffs: [0.04, -0.68, -0.7, -0.66]\n",
            "Payoffs: [0.04, -0.76, -0.68, -0.6]\n",
            "Step: 1710000 Loss: 0.025382164865732193\n",
            "Target Updated\n",
            "Step: 1730000 Loss: 0.02434997819364071\n",
            "Target Updated\n",
            "Episode: 15300/100000 DONE!\n",
            "Payoffs: [0.02, -0.6, -0.72, -0.72]\n",
            "Episode: 14400/100000 DONE!\n",
            "Step: 1720000 Loss: 0.022471729665994644\n",
            "Target Updated\n",
            "Payoffs: [0.0, -0.64, -0.74, -0.66]\n",
            "Step: 1740000 Loss: 0.02775506302714348\n",
            "Target Updated\n",
            "Step: 1730000 Loss: 0.02612219750881195\n",
            "Target Updated\n",
            "Episode: 15400/100000 DONE!\n",
            "Episode: 14500/100000 DONE!\n",
            "Payoffs: [-0.08, -0.66, -0.64, -0.62]\n",
            "Payoffs: [0.0, -0.68, -0.66, -0.66]\n",
            "Step: 1750000 Loss: 0.021571112796664238\n",
            "Target Updated\n",
            "Step: 1740000 Loss: 0.02798941731452942\n",
            "Target Updated\n",
            "Episode: 15500/100000 DONE!\n",
            "Payoffs: [-0.16, -0.52, -0.66, -0.72]\n",
            "Episode: 14600/100000 DONE!\n",
            "Payoffs: [-0.06, -0.74, -0.54, -0.7]\n",
            "Step: 1760000 Loss: 0.032793402671813965\n",
            "Target Updated\n",
            "Step: 1750000 Loss: 0.0287507064640522\n",
            "Target Updated\n",
            "Episode: 15600/100000 DONE!\n",
            "Payoffs: [-0.22, -0.52, -0.7, -0.6]\n",
            "Episode: 14700/100000 DONE!\n",
            "Payoffs: [0.02, -0.7, -0.76, -0.6]\n",
            "Step: 1770000 Loss: 0.028500817716121674\n",
            "Target Updated\n",
            "Step: 1760000 Loss: 0.027361977845430374\n",
            "Target Updated\n",
            "Episode: 15700/100000 DONE!\n",
            "Payoffs: [-0.12, -0.64, -0.64, -0.6]\n",
            "Step: 1780000 Loss: 0.03525914251804352\n",
            "Target Updated\n",
            "Episode: 14800/100000 DONE!\n",
            "Payoffs: [-0.18, -0.56, -0.82, -0.46]\n",
            "Step: 1770000 Loss: 0.024343375116586685\n",
            "Target Updated\n",
            "Episode: 15800/100000 DONE!\n",
            "Payoffs: [-0.12, -0.72, -0.46, -0.72]\n",
            "Step: 1790000 Loss: 0.03368429094552994\n",
            "Target Updated\n",
            "Episode: 14900/100000 DONE!\n",
            "Step: 1780000 Loss: 0.027376610785722733\n",
            "Target Updated\n",
            "Payoffs: [-0.14, -0.72, -0.62, -0.52]\n",
            "Episode: 15900/100000 DONE!\n",
            "Payoffs: [0.04, -0.72, -0.56, -0.76]\n",
            "Step: 1800000 Loss: 0.027724523097276688\n",
            "Target Updated\n",
            "Step: 1790000 Loss: 0.045371972024440765\n",
            "Target Updated\n",
            "Episode: 15000/100000 DONE!\n",
            "Payoffs: [-0.14, -0.6, -0.62, -0.66]\n",
            "Estimator Saved\n",
            "Episode: 16000/100000 DONE!\n",
            "Step: 1810000 Loss: 0.03196348994970322\n",
            "Target Updated\n",
            "Payoffs: [0.04, -0.72, -0.84, -0.5]\n",
            "Estimator Saved\n",
            "Step: 1800000 Loss: 0.02177557907998562\n",
            "Target Updated\n",
            "Episode: 15100/100000 DONE!\n",
            "Payoffs: [-0.14, -0.58, -0.78, -0.52]\n",
            "Step: 1820000 Loss: 0.03198563680052757\n",
            "Target Updated\n",
            "Episode: 16100/100000 DONE!\n",
            "Payoffs: [0.0, -0.76, -0.64, -0.6]\n",
            "Step: 1810000 Loss: 0.027265630662441254\n",
            "Target Updated\n",
            "Episode: 15200/100000 DONE!\n",
            "Payoffs: [0.0, -0.66, -0.7, -0.64]\n",
            "Step: 1830000 Loss: 0.03238769620656967\n",
            "Target Updated\n",
            "Step: 1820000 Loss: 0.015055627562105656\n",
            "Target Updated\n",
            "Episode: 16200/100000 DONE!\n",
            "Payoffs: [-0.18, -0.6, -0.52, -0.74]\n",
            "Episode: 15300/100000 DONE!\n",
            "Payoffs: [-0.16, -0.62, -0.54, -0.68]\n",
            "Step: 1840000 Loss: 0.025350408628582954\n",
            "Target Updated\n",
            "Step: 1830000 Loss: 0.024687794968485832\n",
            "Target Updated\n",
            "Episode: 16300/100000 DONE!\n",
            "Payoffs: [-0.02, -0.72, -0.66, -0.6]\n",
            "Step: 1850000 Loss: 0.03824805095791817\n",
            "Target Updated\n",
            "Episode: 15400/100000 DONE!\n",
            "Step: 1840000 Loss: 0.02671245113015175\n",
            "Target Updated\n",
            "Payoffs: [-0.04, -0.68, -0.64, -0.66]\n",
            "Episode: 16400/100000 DONE!\n",
            "Payoffs: [-0.02, -0.7, -0.68, -0.6]\n",
            "Step: 1860000 Loss: 0.01688990369439125\n",
            "Target Updated\n",
            "Step: 1850000 Loss: 0.021082336083054543\n",
            "Target Updated\n",
            "Episode: 15500/100000 DONE!\n",
            "Episode: 16500/100000 DONE!\n",
            "Payoffs: [-0.06, -0.66, -0.68, -0.62]\n",
            "Payoffs: [0.02, -0.74, -0.62, -0.66]\n",
            "Step: 1870000 Loss: 0.04924066364765167\n",
            "Target Updated\n",
            "Step: 1860000 Loss: 0.025887517258524895\n",
            "Target Updated\n",
            "Episode: 16600/100000 DONE!\n",
            "Step: 1880000 Loss: 0.02990097366273403\n",
            "Target Updated\n",
            "Payoffs: [-0.2, -0.54, -0.6, -0.68]\n",
            "Episode: 15600/100000 DONE!\n",
            "Payoffs: [0.02, -0.62, -0.68, -0.72]\n",
            "Step: 1870000 Loss: 0.038716718554496765\n",
            "Target Updated\n",
            "Episode: 16700/100000 DONE!\n",
            "Step: 1890000 Loss: 0.030093159526586533\n",
            "Target Updated\n",
            "Payoffs: [-0.08, -0.56, -0.66, -0.72]\n",
            "Step: 1880000 Loss: 0.028578825294971466\n",
            "Target Updated\n",
            "Episode: 15700/100000 DONE!\n",
            "Payoffs: [-0.06, -0.64, -0.66, -0.68]\n",
            "Episode: 16800/100000 DONE!\n",
            "Step: 1900000 Loss: 0.03669819235801697\n",
            "Target Updated\n",
            "Payoffs: [-0.08, -0.7, -0.68, -0.56]\n",
            "Step: 1890000 Loss: 0.02413015067577362\n",
            "Target Updated\n",
            "Step: 1910000 Loss: 0.03336125984787941\n",
            "Target Updated\n",
            "Episode: 16900/100000 DONE!\n",
            "Episode: 15800/100000 DONE!\n",
            "Payoffs: [0.02, -0.7, -0.7, -0.62]\n",
            "Payoffs: [-0.12, -0.74, -0.62, -0.58]\n",
            "Step: 1900000 Loss: 0.027777502313256264\n",
            "Target Updated\n",
            "Step: 1920000 Loss: 0.028977207839488983\n",
            "Target Updated\n",
            "Episode: 17000/100000 DONE!\n",
            "Payoffs: [-0.06, -0.7, -0.64, -0.62]\n",
            "Estimator Saved\n",
            "Step: 1910000 Loss: 0.019653521478176117\n",
            "Target Updated\n",
            "Episode: 15900/100000 DONE!\n",
            "Payoffs: [0.02, -0.64, -0.72, -0.66]\n",
            "Step: 1930000 Loss: 0.03268361836671829\n",
            "Target Updated\n",
            "Episode: 17100/100000 DONE!\n",
            "Payoffs: [0.04, -0.62, -0.8, -0.64]\n",
            "Step: 1920000 Loss: 0.030520156025886536\n",
            "Target Updated\n",
            "Step: 1940000 Loss: 0.02939615398645401\n",
            "Target Updated\n",
            "Episode: 16000/100000 DONE!\n",
            "Episode: 17200/100000 DONE!\n",
            "Payoffs: [0.08, -0.74, -0.68, -0.66]\n",
            "Payoffs: [-0.1, -0.64, -0.74, -0.54]\n",
            "Estimator Saved\n",
            "Step: 1930000 Loss: 0.02270064689218998\n",
            "Target Updated\n",
            "Step: 1950000 Loss: 0.021712755784392357\n",
            "Target Updated\n",
            "Process Process-2:\n",
            "Process Process-1:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-23-e238cde2ba2b>\", line 10, in train_DQN_multiprocessing_wrapper\n",
            "    pretraining_file_name=folder_path+'/RL_pretrained_1.pt',\n",
            "  File \"<ipython-input-12-f3358fe2b027>\", line 42, in train_DQN\n",
            "    loss = agent.feed(prev_state_actions[id], 0, state_actions)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-23-e238cde2ba2b>\", line 5, in train_DQN_multiprocessing_wrapper\n",
            "    pretraining_file_name=folder_path+'/RL_pretrained_0.pt',\n",
            "  File \"<ipython-input-8-0429925c4c15>\", line 72, in feed\n",
            "    return self.train()\n",
            "  File \"<ipython-input-12-f3358fe2b027>\", line 56, in train_DQN\n",
            "    loss = agent.feed(state_action, reward, terminal)\n",
            "  File \"<ipython-input-8-0429925c4c15>\", line 92, in train\n",
            "    best_state_actions_list = [sa_s[q_sa_s.argmax()] for sa_s, q_sa_s in zip(next_state_actions_list, q_values_next_learner_split)]\n",
            "  File \"<ipython-input-8-0429925c4c15>\", line 92, in <listcomp>\n",
            "    best_state_actions_list = [sa_s[q_sa_s.argmax()] for sa_s, q_sa_s in zip(next_state_actions_list, q_values_next_learner_split)]\n",
            "  File \"<ipython-input-8-0429925c4c15>\", line 72, in feed\n",
            "    return self.train()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b496761e5b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mp0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mp0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_agent(\n",
        "    environment,\n",
        "    random_seed=6450,\n",
        "    ckpt=None,\n",
        "    prefix=\"dqn\",\n",
        "    layers=3,\n",
        "    layer_size=512,\n",
        "    complexity=0,\n",
        "    pretraining=0,\n",
        "):\n",
        "    name = f\"{prefix}{layer_size}_{layers}{complexity}_{pretraining}\"  # 0 is for no pretraining\n",
        "    if ckpt is None:\n",
        "        import glob\n",
        "\n",
        "        ckpt = f\"{name}/{name}_*.pt\"\n",
        "        ckpts = glob.glob(ckpt)\n",
        "        ckpts.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "        if len(ckpt) == 0:\n",
        "            raise ValueError(\"No checkpoint found\")\n",
        "        ckpt = ckpts[-1]\n",
        "        print(ckpt)\n",
        "        # raise ValueError(f'No checkpoint specified, using {ckpt}')\n",
        "    else:\n",
        "        ckpt = f\"{name}/{name}_{ckpt}.pt\"\n",
        "\n",
        "    state_action_dims = environment.state_dimensions + environment.action_dimensions\n",
        "    cls = (\n",
        "        DQN_Agent if prefix == \"dqn\" else DMC_Agent if prefix == \"dmc\" else Random_Agent\n",
        "    )\n",
        "    agent: Union[DQN_Agent, DMC_Agent, Brute_Agent, Random_Agent] = cls(\n",
        "        state_action_dims=state_action_dims,\n",
        "        random_seed=random_seed,\n",
        "        hidden_dims=[layer_size] * layers,\n",
        "        name=name,\n",
        "    )\n",
        "    if isinstance(agent, DQN_Agent):\n",
        "        agent.learner_estimator.load_model(ckpt)\n",
        "        agent.exploration_rate = 0\n",
        "    elif isinstance(agent, DMC_Agent):\n",
        "        agent.estimator.load_model(ckpt)\n",
        "        agent.exploration_rate = 0\n",
        "    # agent = Brute_Agent(environment, num_sims=10, time_limit=5)\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "def eval_a_model(\n",
        "    ckpt=None,\n",
        "    prefix=\"dqn\",\n",
        "    layers=3,\n",
        "    layer_size=512,\n",
        "    complexity=0,\n",
        "    pretraining=0,\n",
        "    num_games=10000,\n",
        "    order_perm=None,\n",
        "    char_perm=None,\n",
        "    random_seed=6450,\n",
        "):\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        handlers=[\n",
        "            logging.FileHandler(\"pretraining.log\"),\n",
        "            logging.StreamHandler(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    environment = DungeonMayhem_Environment(\n",
        "        random_seed=random_seed, encoding_complexity=complexity\n",
        "    )\n",
        "    agent = load_agent(\n",
        "        environment,\n",
        "        ckpt=ckpt,\n",
        "        random_seed=random_seed,\n",
        "        prefix=prefix,\n",
        "        layers=layers,\n",
        "        layer_size=layer_size,\n",
        "        complexity=complexity,\n",
        "        pretraining=pretraining,\n",
        "    )\n",
        "\n",
        "    # agents = [Random_Agent(random_seed)] * 4\n",
        "    agents = [agent] + [Random_Agent(random_seed)] * 3\n",
        "    payoffs = tournament(\n",
        "        environment,\n",
        "        agents,\n",
        "        num_games=num_games,\n",
        "        order_perm=order_perm,\n",
        "        char_perm=char_perm,\n",
        "    )\n",
        "    logging.info(\"Payoffs {}\".format(payoffs))\n",
        "    return (ckpt, payoffs)\n",
        "\n",
        "\n",
        "def train_a_model(\n",
        "    prefix, size=512, num=3, complexity=0, pretraining=0, load_file=None, func=None\n",
        "):\n",
        "    if func == None:\n",
        "        if prefix == \"dqn\":\n",
        "            func = train_DQN\n",
        "        elif prefix == \"dmc\":\n",
        "            func = train_DMC\n",
        "    return func(\n",
        "        hidden_dims=[size] * num,\n",
        "        encoding_complexity=complexity,\n",
        "        name=f\"{prefix}{size}_{num}{complexity}_{pretraining}\",\n",
        "        pretraining_file_name=load_file,\n",
        "    )\n",
        "\n",
        "\n",
        "def round_robin(env, agents, num_games=100):\n",
        "    agents += [Random_Agent(random_seed=6450)] * 4\n",
        "    from itertools import permutations\n",
        "\n",
        "    for order_perm in permutations(range(4)):\n",
        "        for char_perm in permutations(range(4)):\n",
        "            payoffs = tournament(\n",
        "                env,\n",
        "                agents,\n",
        "                num_games=num_games,\n",
        "                order_perm=order_perm,\n",
        "                char_perm=char_perm,\n",
        "            )\n",
        "            logging.info(\"Payoffs {} {} = {}\".format(order_perm, char_perm, payoffs))\n",
        "            yield (order_perm, char_perm, payoffs)\n",
        "\n",
        "def round_robin_helper(layers=3, layer_size=512, complexity=0, pretraining=0, num_games=100, all_rando=False):\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        handlers=[\n",
        "            logging.FileHandler(\"pretraining.log\"),\n",
        "            logging.StreamHandler(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    environment = DungeonMayhem_Environment(random_seed=6450)\n",
        "    payoffs = round_robin(\n",
        "        environment,\n",
        "        [] if all_rando else\n",
        "        [\n",
        "            load_agent(\n",
        "                environment,\n",
        "                prefix=\"dqn\",\n",
        "                layers=layers,\n",
        "                layer_size=layer_size,\n",
        "                complexity=complexity,\n",
        "                pretraining=pretraining,\n",
        "            ),\n",
        "            load_agent(\n",
        "                environment,\n",
        "                prefix=\"dmc\",\n",
        "                layers=layers,\n",
        "                layer_size=layer_size,\n",
        "                complexity=complexity,\n",
        "                pretraining=pretraining,\n",
        "            )\n",
        "        ],\n",
        "        num_games=num_games,\n",
        "    )\n",
        "    with open(f\"tournament_{layer_size}_{layers}{complexity}_{pretraining}.pickle\", \"wb\") as fp:\n",
        "        pickle.dump(list(payoffs), fp)\n",
        "\n",
        "# rr_num_games = 1000\n",
        "# round_robin_helper(layers=3, layer_size=512, complexity=0, pretraining=0, num_games=rr_num_games)\n",
        "# round_robin_helper(layers=3, layer_size=256, complexity=0, pretraining=0, num_games=rr_num_games)\n",
        "# round_robin_helper(layers=4, layer_size=512, complexity=0, pretraining=0, num_games=rr_num_games)\n",
        "# round_robin_helper(layers=5, layer_size=512, complexity=0, pretraining=0, num_games=rr_num_games)\n",
        "\n",
        "payoffs = []\n",
        "single_model_num = 10000\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=3, layer_size=512, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=3, layer_size=512, complexity=1))\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=3, layer_size=512, complexity=2))\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=4, layer_size=512, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=5, layer_size=512, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=3, layer_size=512, complexity=0, pretraining=50,))\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=3, layer_size=512, complexity=0, pretraining=100,))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=3, layer_size=512, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=3, layer_size=512, complexity=1))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=3, layer_size=512, complexity=2))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=4, layer_size=512, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=5, layer_size=512, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=3, layer_size=512, complexity=0, pretraining=50,))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=3, layer_size=512, complexity=0, pretraining=100,))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=3, layer_size=256, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dmc\", num_games=single_model_num, layers=4, layer_size=256, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=3, layer_size=256, complexity=0))\n",
        "payoffs.append( eval_a_model( prefix=\"dqn\", num_games=single_model_num, layers=4, layer_size=256, complexity=0))\n",
        "with open(\"testing.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(payoffs, fp)\n",
        "\n",
        "\n",
        "# agent, loss_list, payoffs_list = train_a_model(\"dmc\", 2048, 3, 0)\n",
        "# agent, loss_list, payoffs_list = train_a_model(\"dmc\", 256, 3, 0)\n",
        "# agent, loss_list, payoffs_list = train_a_model(\"dqn\", 256, 3, 0)\n",
        "# agent, loss_list, payoffs_list = train_a_model(\"dmc\", 256, 4, 0)\n",
        "# agent, loss_list, payoffs_list = train_a_model(\"dqn\", 256, 4, 0)\n",
        "# agent, loss_list, payoffs_list = train_a_model(\"dqn\", 128, 4, 0) # TODO\n",
        "# agent, loss_list, payoffs_list = train_a_model(\"dmc\", 128, 4, 0)\n",
        "# raise ValueError(\"Training finished\")\n"
      ],
      "metadata": {
        "id": "7986TkT0deOV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QmDG5l7INqSC",
        "Yxywgcx7h7gU",
        "cLStzHP5kWSh",
        "SZciNAS6xA9o",
        "wNq9xIhzxCRK",
        "8cY3pw5qHX1_",
        "tNQPnsdi5TdD",
        "H8ArQ4Dd4QES",
        "y2n094JYZ-dG",
        "gu_MVfXt4jwk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}